{"cells":[{"cell_type":"code","execution_count":9,"metadata":{"collapsed":true,"id":"eugbzc9F--tI","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1755384116293,"user_tz":180,"elapsed":11137,"user":{"displayName":"Aleksei Korablev","userId":"02837062072924939756"}},"outputId":"baa7ae7a-9548-46ce-b197-238df96f1c4d"},"outputs":[{"output_type":"stream","name":"stdout","text":["W: Skipping acquire of configured file 'main/source/Sources' as repository 'https://r2u.stat.illinois.edu/ubuntu jammy InRelease' does not seem to provide it (sources.list entry misspelt?)\n"]}],"source":["# ===========================\n","# Colab setup\n","# ===========================\n","\n","!pip install -q openai-whisper pydub requests iso8601 reportlab\n","!apt-get update -qq && apt-get install -y -qq ffmpeg"]},{"cell_type":"code","execution_count":10,"metadata":{"id":"iZueWnhcXOom","executionInfo":{"status":"ok","timestamp":1755384116300,"user_tz":180,"elapsed":2,"user":{"displayName":"Aleksei Korablev","userId":"02837062072924939756"}}},"outputs":[],"source":["# ===========================\n","# Imports\n","# ===========================\n","import whisper\n","import json\n","import datetime\n","from datetime import timedelta, timezone\n","from pathlib import Path\n","import torch\n","from pydub import AudioSegment\n","import numpy as np\n","import requests\n","import re\n","import iso8601\n","import csv\n","import subprocess\n","from IPython.display import clear_output\n","from tqdm.notebook import tqdm  # For Jupyter/Colab\n","\n","# ReportLab\n","from reportlab.lib import colors\n","from reportlab.lib.pagesizes import letter\n","from reportlab.platypus import SimpleDocTemplate, Table, TableStyle, Paragraph, Spacer, PageBreak\n","from reportlab.lib.styles import getSampleStyleSheet, ParagraphStyle\n","from reportlab.lib.units import inch"]},{"cell_type":"code","source":["# ===========================\n","# Helpers — cURL, paths, time\n","# ===========================\n","def extract_ids_from_curl(curl_string: str):\n","    \"\"\"\n","    Pull course_id, section_id, class_id from the Referer in the cURL (if present).\n","    Fallback: try to find classes/<digits> anywhere in the cURL.\n","    \"\"\"\n","    out = {\"course_id\": None, \"section_id\": None, \"class_id\": None, \"referer\": None}\n","    # Referer header\n","    ref_match = re.search(r\"-H\\s+'referer:\\s*(.*?)'\", curl_string, re.IGNORECASE)\n","    if ref_match:\n","        ref = ref_match.group(1)\n","        out[\"referer\"] = ref\n","        m = re.search(r\"/courses/(\\d+)/sections/(\\d+)/classes/(\\d+)\", ref)\n","        if m:\n","            out[\"course_id\"], out[\"section_id\"], out[\"class_id\"] = m.group(1), m.group(2), m.group(3)\n","\n","    if not out[\"class_id\"]:\n","        m2 = re.search(r\"/classes/(\\d+)\", curl_string)\n","        if m2:\n","            out[\"class_id\"] = m2.group(1)\n","    return out\n","\n","def derive_class_link_from_curl(curl_string: str, course_id: str = None, section_id: str = None, class_id: str = None):\n","    \"\"\"\n","    If referer exists, return it; else build link if we have ids; else empty string.\n","    \"\"\"\n","    ref_match = re.search(r\"-H\\s+'referer:\\s*(.*?)'\", curl_string, re.IGNORECASE)\n","    if ref_match:\n","        return ref_match.group(1)\n","    if course_id and section_id and class_id:\n","        return f\"https://forum.minerva.edu/app/courses/{course_id}/sections/{section_id}/classes/{class_id}\"\n","    if class_id:\n","        return f\"https://forum.minerva.edu/app/classes/{class_id}\"\n","    return \"\"\n","\n","def _safe_date(date_str):\n","    # return YYYY-MM-DD from ISO8601, or \"\"\n","    if not date_str:\n","        return \"\"\n","    try:\n","        return date_str.split('T')[0]\n","    except Exception:\n","        return \"\"\n","\n","def _fmt_mmss(seconds_float):\n","    if seconds_float is None:\n","        return \"\"\n","    seconds = max(0, int(seconds_float))\n","    m, s = divmod(seconds, 60)\n","    return f\"{m:02d}:{s:02d}\"\n","\n","def normalize_sentence_spacing(text: str) -> str:\n","    \"\"\"\n","    Fixes missing spaces after sentence punctuation and collapses over-spacing.\n","    Handles cases like \"taught CS51.So you've\" -> \"taught CS51. So you've\"\n","    \"\"\"\n","    if not text:\n","        return text\n","    # Ensure space after . ! ? when followed by letter/quote/number\n","    text = re.sub(r'([.!?])(?=[A-Za-z0-9\"\\'])', r'\\1 ', text)\n","    # Collapse multiple spaces\n","    text = re.sub(r'\\s{2,}', ' ', text)\n","    # Trim space before punctuation\n","    text = re.sub(r'\\s+([,.!?;:])', r'\\1', text)\n","    # Make sure quotes then letter also have space before if needed: already covered by first rule most times\n","    return text.strip()\n","\n","_ZWSP = \"\\u200b\"\n","def soft_break_long_token(s: str, every: int = 14) -> str:\n","    \"\"\"\n","    Insert zero-width breaks into very long tokens to avoid PDF table overlap.\n","    Keeps spaces intact; only breaks long runs of non-space characters.\n","    \"\"\"\n","    if not s:\n","        return s\n","    parts = []\n","    for token in re.split(r\"(\\s+)\", s):\n","        if token.strip() == \"\":\n","            parts.append(token)\n","        else:\n","            # insert breaks every N characters\n","            chunks = [token[i:i+every] for i in range(0, len(token), every)]\n","            parts.append(_ZWSP.join(chunks))\n","    return \"\".join(parts)\n","\n","def clean_curl(curl_string):\n","   headers = {}\n","\n","   # Extract headers with -H flag\n","   header_matches = re.findall(r\"-H ['\\\"](.*?): (.*?)['\\\"]\", curl_string)\n","   for name, value in header_matches:\n","       headers[name] = value\n","\n","   # Extract cookies with -b flag\n","   cookie_match = re.search(r\"-b ['\\\"](.*?)['\\\"]\", curl_string)\n","   if cookie_match:\n","       cookie_str = cookie_match.group(1)\n","       headers['Cookie'] = cookie_str\n","\n","   # Ensure we look like XHR JSON\n","   headers.setdefault(\"accept\", \"application/json, text/javascript, */*; q=0.01\")\n","   headers.setdefault(\"x-requested-with\", \"XMLHttpRequest\")\n","   return headers"],"metadata":{"id":"ZBI8zO3oC7nU","executionInfo":{"status":"ok","timestamp":1755384116303,"user_tz":180,"elapsed":2,"user":{"displayName":"Aleksei Korablev","userId":"02837062072924939756"}}},"execution_count":11,"outputs":[]},{"cell_type":"code","execution_count":12,"metadata":{"id":"Zz_o4ed-Unr8","executionInfo":{"status":"ok","timestamp":1755384116314,"user_tz":180,"elapsed":10,"user":{"displayName":"Aleksei Korablev","userId":"02837062072924939756"}}},"outputs":[],"source":["# ===========================\n","# Audio preprocessor\n","# ===========================\n","class AudioPreprocessor:\n","    @staticmethod\n","    def validate_and_fix_file(file_path: str) -> str:\n","        \"\"\"\n","        Validates and preprocesses audio files for optimal transcription.\n","        Supports direct URLs (downloads to /content/input_downloaded.*).\n","        For MP4 files, converts to WAV for Whisper.\n","        \"\"\"\n","        print(f\"Validating file or URL: {file_path}\")\n","\n","        # If URL, download\n","        if isinstance(file_path, str) and re.match(r\"^https?://\", file_path.strip(), re.IGNORECASE):\n","            try:\n","                print(\"Detected URL — downloading...\")\n","                resp = requests.get(file_path, stream=True, timeout=60)\n","                resp.raise_for_status()\n","                suffix = \".mp4\" if \".mp4\" in file_path.lower() else (\".mp3\" if \".mp3\" in file_path.lower() else \".bin\")\n","                dl_path = \"/content/input_downloaded\" + suffix\n","                with open(dl_path, \"wb\") as f:\n","                    for chunk in resp.iter_content(chunk_size=1024 * 1024):\n","                        if chunk:\n","                            f.write(chunk)\n","                print(f\"Downloaded to: {dl_path}\")\n","                file_path = dl_path\n","            except Exception as e:\n","                raise RuntimeError(f\"Failed to download media: {e}\")\n","\n","        # Local/Downloaded path must exist now\n","        if not Path(file_path).exists():\n","            raise FileNotFoundError(f\"File not found: {file_path}\")\n","\n","        try:\n","            if file_path.lower().endswith('.mp4'):\n","                print(f\"Converting MP4 → Whisper-optimized WAV...\")\n","                return AudioPreprocessor._convert_to_whisper_wav(file_path)\n","            elif file_path.lower().endswith(('.mp3', '.m4a', '.aac', '.ogg', '.wav')):\n","                print(f\"Normalizing to Whisper-optimized WAV...\")\n","                return AudioPreprocessor._convert_to_whisper_wav(file_path)\n","            else:\n","                print(\"Unknown format — attempting Python fallback decode...\")\n","                return AudioPreprocessor._python_extract_audio(file_path)\n","        except Exception as e:\n","            print(f\"Error during file processing: {str(e)}\")\n","            raise\n","\n","    @staticmethod\n","    def _convert_to_whisper_wav(audio_path: str) -> str:\n","        \"\"\"Convert any audio file to WAV format optimized for Whisper model\"\"\"\n","        wav_path = audio_path.rsplit('.', 1)[0] + '.wav'\n","        try:\n","            result = subprocess.run([\n","                'ffmpeg','-y','-i', audio_path,\n","                '-acodec','pcm_s16le','-ar','16000','-ac','1', wav_path\n","            ], capture_output=True, text=True, check=False)\n","            if result.returncode != 0:\n","                raise RuntimeError(result.stderr or \"ffmpeg failed\")\n","            print(f\"Created: {wav_path}\")\n","            return wav_path\n","        except Exception as e:\n","            raise RuntimeError(f\"Failed to convert {audio_path} → WAV: {e}\")\n","\n","    @staticmethod\n","    def _python_extract_audio(file_path: str) -> str:\n","        \"\"\"\n","        Fallback: use PyDub to decode & write 16kHz mono WAV.\n","        \"\"\"\n","        print(\"Attempting Python-based audio extraction...\")\n","        wav_path = file_path.rsplit('.', 1)[0] + '_extracted.wav'\n","        audio = AudioSegment.from_file(file_path)\n","        audio = audio.set_frame_rate(16000).set_channels(1).set_sample_width(2)\n","        audio.export(wav_path, format=\"wav\")\n","        if not Path(wav_path).exists() or Path(wav_path).stat().st_size == 0:\n","            raise RuntimeError(\"Python audio extraction produced empty file\")\n","        print(f\"Created: {wav_path}\")\n","        return wav_path"]},{"cell_type":"code","execution_count":13,"metadata":{"id":"DASfAmyYPzWu","executionInfo":{"status":"ok","timestamp":1755384116316,"user_tz":180,"elapsed":1,"user":{"displayName":"Aleksei Korablev","userId":"02837062072924939756"}}},"outputs":[],"source":["# ===========================\n","# Whisper transcription\n","# ===========================\n","class TranscriptionProcessor:\n","    def __init__(self, segment_length=14400):\n","        self.device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n","        print(f\"Using device: {self.device}\")\n","        if self.device == \"cuda\":\n","            torch.backends.cuda.matmul.allow_tf32 = True\n","            torch.backends.cudnn.benchmark = True\n","            torch.backends.cudnn.allow_tf32 = True\n","            torch.backends.cudnn.deterministic = False\n","            torch.cuda.empty_cache()\n","        print(\"Loading Whisper model...\")\n","        self.model = whisper.load_model(\"medium\").to(self.device)\n","        if self.device == \"cuda\":\n","            self.model = self.model.half()\n","        self.segment_length = segment_length\n","\n","    def transcribe(self, audio_path, class_id):\n","        print(f\"Processing audio to generate transcript JSON...\")\n","        try:\n","            audio = AudioSegment.from_file(audio_path)\n","            total_duration = len(audio) / 1000\n","            print(f\"Total duration: {timedelta(seconds=int(total_duration))}\")\n","\n","            all_segments = []\n","            segment_times = range(0, int(total_duration), self.segment_length)\n","\n","            for start_time in tqdm(segment_times, desc=\"Processing segments\", unit=\"segment\"):\n","                duration = min(self.segment_length, total_duration - start_time)\n","                segment = audio[start_time*1000:(start_time+duration)*1000]\n","                temp_path = f\"/content/temp_segment_{start_time}.wav\"\n","                segment.export(temp_path, format=\"wav\")\n","\n","                try:\n","                    # Updated deprecation: use torch.amp.autocast(\"cuda\", ...)\n","                    use_amp = (self.device == \"cuda\")\n","                    ctx_mgr = torch.amp.autocast(\"cuda\") if use_amp else nullcontext()\n","                except Exception:\n","                    # Fallback if torch.amp not available\n","                    class _Dummy:\n","                        def __enter__(self): pass\n","                        def __exit__(self, *a): pass\n","                    ctx_mgr = _Dummy()\n","\n","                try:\n","                    with ctx_mgr:\n","                        result = self.model.transcribe(\n","                            temp_path,\n","                            word_timestamps=True,\n","                            language='en',\n","                            task='transcribe',\n","                            fp16=(self.device==\"cuda\"),\n","                            condition_on_previous_text=True,\n","                            initial_prompt=\"This is a university lecture.\"\n","                        )\n","\n","                    for seg in result[\"segments\"]:\n","                        seg_start = float(seg[\"start\"]) + start_time\n","                        seg_end   = float(seg[\"end\"]) + start_time\n","                        words = []\n","                        for w in seg.get(\"words\", []):\n","                            words.append({\n","                                \"word\": w[\"word\"].strip(),\n","                                \"start\": float(w[\"start\"]) + start_time,\n","                                \"end\": float(w[\"end\"]) + start_time\n","                            })\n","                        all_segments.append({\n","                            \"start\": seg_start,\n","                            \"end\": seg_end,\n","                            \"text\": seg[\"text\"].strip(),\n","                            \"words\": words\n","                        })\n","\n","                finally:\n","                    try:\n","                        Path(temp_path).unlink(missing_ok=True)\n","                    except:\n","                        pass\n","                    if self.device == \"cuda\":\n","                        torch.cuda.empty_cache()\n","\n","            if not all_segments:\n","                print(\"Warning: No segments were transcribed.\")\n","                return None\n","\n","            transcript_path = f\"/content/session_{class_id}_transcript.json\"\n","            with open(transcript_path, 'w', encoding='utf-8') as f:\n","                json.dump({\"segments\": sorted(all_segments, key=lambda x: x[\"start\"])}, f, indent=2)\n","\n","            print(f\"Transcript JSON saved to: {transcript_path}\")\n","            return transcript_path\n","\n","        except Exception as e:\n","            print(f\"Error in transcription process: {str(e)}\")\n","            raise\n","\n","# tiny helper for autocast fallback\n","from contextlib import nullcontext"]},{"cell_type":"code","execution_count":14,"metadata":{"id":"yAdjGA9VVV_2","executionInfo":{"status":"ok","timestamp":1755384116317,"user_tz":180,"elapsed":2,"user":{"displayName":"Aleksei Korablev","userId":"02837062072924939756"}}},"outputs":[],"source":["# ===========================\n","# Forum data (events, voice, attendance)\n","# ===========================\n","def get_forum_events(class_id, headers, raw_curl):\n","    print(\"Fetching class and event data from Forum...\")\n","    try:\n","        # Class meta\n","        class_url = f'https://forum.minerva.edu/api/v1/class_grader/classes/{class_id}'\n","        r = requests.get(class_url, headers=headers, timeout=30)\n","        if r.status_code != 200:\n","            print(f\"Class data error: {r.status_code}\\n{r.text[:400]}\")\n","            raise RuntimeError(f\"Failed to access class data. Status code: {r.status_code}\")\n","        data = r.json()\n","\n","        session_title = data.get('title') or f\"Session {class_id}\"\n","        section_title = (data.get('section') or {}).get('title', '')  # e.g. \"Terrana, MW@09:00AM San Francisco\"\n","        course_obj = (data.get('section') or {}).get('course') or {}\n","        course_code  = course_obj.get('course-code', '')\n","        course_title = course_obj.get('title', '')\n","        class_type   = data.get('type', '')\n","\n","        # Recording window\n","        rec = (data.get('recording-sessions') or [{}])[0]\n","        recording_start = rec.get('recording-started')\n","        recording_end   = rec.get('recording-ended')\n","\n","        # Guess schedule from section_title \"Terrana, MW@09:00AM San Francisco\"\n","        schedule_guess = ''\n","        if isinstance(section_title, str) and ',' in section_title:\n","            parts = [p.strip() for p in section_title.split(',', 1)]\n","            schedule_guess = parts[1] if len(parts) > 1 else ''\n","\n","        ids = extract_ids_from_curl(raw_curl)\n","        course_id  = ids.get(\"course_id\")\n","        section_id = ids.get(\"section_id\")\n","        class_link = derive_class_link_from_curl(raw_curl, course_id, section_id, str(class_id))\n","\n","        # ---- Events ----\n","        events_url = f'https://forum.minerva.edu/api/v1/class_grader/classes/{class_id}/class-events'\n","        r = requests.get(events_url, headers=headers, timeout=30)\n","        if r.status_code != 200:\n","            print(f\"Class events error: {r.status_code}\\n{r.text[:400]}\")\n","            raise RuntimeError(f\"Failed to access class events. Status code: {r.status_code}\")\n","\n","        events = r.json()\n","        if not isinstance(events, list):\n","            raise ValueError(\"No valid class events returned from API\")\n","\n","        # Parse voice & timeline\n","        voice_events = []\n","        timeline_segments = []\n","\n","        if not recording_start:\n","            raise KeyError(\"No recording-started found in class data\")\n","        ref_time = iso8601.parse_date(recording_start)\n","\n","        for ev in events:\n","            et = ev.get('event-type')\n","            try:\n","                if et == 'voice':\n","                    duration_ms = (ev.get('event-data') or {}).get('duration', 0)\n","                    duration = duration_ms / 1000.0\n","                    if duration >= 1:\n","                        start_time = iso8601.parse_date(ev['start-time'])\n","                        end_time   = iso8601.parse_date(ev['end-time'])\n","                        actor = ev.get('actor') or {}\n","                        voice_events.append({\n","                            'start': (start_time - ref_time).total_seconds(),\n","                            'end': (end_time - ref_time).total_seconds(),\n","                            'duration': duration,\n","                            'speaker': {\n","                                'id': actor.get('id'),\n","                                'first_name': actor.get('first-name'),\n","                                'last_name':  actor.get('last-name')\n","                            }\n","                        })\n","                elif et == 'timeline-segment':\n","                    start_time = iso8601.parse_date(ev['start-time'])\n","                    seg = (ev.get('event-data') or {})\n","                    timeline_segments.append({\n","                        'abs_start': ev['start-time'],\n","                        'offset_seconds': (start_time - ref_time).total_seconds(),\n","                        'section': seg.get('timeline-section-title', ''),\n","                        'title':   seg.get('timeline-segment-title', ''),\n","                    })\n","            except KeyError:\n","                continue\n","\n","        timeline_segments.sort(key=lambda x: x['offset_seconds'])\n","\n","        # Attempt attendance from class-users (best-effort)\n","        attendance = []\n","        for cu in data.get('class-users', []):\n","            role = (cu.get('role') or \"\").lower()\n","            if role == 'student':\n","                u = cu.get('user') or {}\n","                name = f\"{(u.get('first-name') or '').strip()} {(u.get('last-name') or '').strip()}\".strip()\n","                uid  = u.get('id') or cu.get('user-id')\n","                # infer absence if possible; default = present\n","                abs_flag = cu.get('absent')\n","                if abs_flag is None:\n","                    att = cu.get('attended')\n","                    abs_flag = (False if att is None else (not bool(att)))\n","                attendance.append({'name': name or f\"ID {uid}\", 'id': uid, 'absent': bool(abs_flag)})\n","\n","        # Build class meta (NO instructor field)\n","        # Also include class_id, course/section IDs and link for headers\n","        class_meta = {\n","            'class_id': str(class_id),\n","            'session_title': session_title,\n","            'course_code': course_code,\n","            'course_title': course_title,\n","            'section_title': section_title,   # \"Terrana, MW@09:00AM San Francisco\"\n","            'schedule': schedule_guess,\n","            'class_type': class_type,\n","            'recording_start': recording_start,\n","            'recording_end': recording_end,\n","            'course_id': course_id,\n","            'section_id': section_id,\n","            'class_link': class_link\n","        }\n","\n","        events_data = {\n","            'class_id': class_id,\n","            'class_meta': class_meta,\n","            'voice_events': voice_events,\n","            'timeline_segments': timeline_segments,\n","            'attendance': attendance\n","        }\n","        temp_events_path = f\"/content/session_{class_id}_events.json\"\n","        with open(temp_events_path, 'w', encoding='utf-8') as f:\n","            json.dump(events_data, f, indent=2)\n","\n","        print(f\"Processed voice events: {len(voice_events)}; timeline segments: {len(timeline_segments)}; attendance: {len(attendance)}\")\n","        return events_data\n","\n","    except Exception as e:\n","        print(f\"Error fetching Forum events: {e}\")\n","        raise"]},{"cell_type":"code","execution_count":15,"metadata":{"id":"nFRSWCdOSfo_","executionInfo":{"status":"ok","timestamp":1755384116367,"user_tz":180,"elapsed":50,"user":{"displayName":"Aleksei Korablev","userId":"02837062072924939756"}}},"outputs":[],"source":["# ===========================\n","# Compile (PDF / CSV)\n","# ===========================\n","def _build_speaker_window_map(events_data, privacy_mode: str):\n","    \"\"\"\n","    Build a map of (start,end) -> display_name based on privacy mode (names/ids).\n","    \"\"\"\n","    speaker_map = {}\n","    for ev in events_data.get('voice_events', []):\n","        fn = (ev['speaker'].get('first_name') or '').strip()\n","        ln = (ev['speaker'].get('last_name') or '').strip()\n","        uid = ev['speaker'].get('id')\n","        name = (f\"{fn} {ln}\".strip() or \"Professor\")\n","        if privacy_mode == \"ids\" and uid:\n","            disp = f\"ID {uid}\"\n","        else:\n","            disp = name\n","        speaker_map[(ev['start'], ev['end'])] = disp\n","    return speaker_map\n","\n","def compile_transcript_to_pdf(class_id, headers, privacy_mode=\"names\"):\n","    try:\n","        # Load JSONs\n","        with open(f\"/content/session_{class_id}_transcript.json\", 'r') as f:\n","            transcript_data = json.load(f)\n","        with open(f\"/content/session_{class_id}_events.json\", 'r') as f:\n","            events_data = json.load(f)\n","\n","        class_meta = events_data.get('class_meta', {})\n","        timeline_segments = events_data.get('timeline_segments', [])\n","        attendance = events_data.get('attendance', [])\n","\n","        # Speaker mapping\n","        speaker_map = _build_speaker_window_map(events_data, privacy_mode)\n","\n","        def find_speaker_at_time(time_point):\n","            for (start, end), speaker in speaker_map.items():\n","                if start <= time_point <= end:\n","                    return speaker\n","            return \"Professor\"  # generic fallback\n","\n","        # Combine consecutive segments by same speaker\n","        compiled_entries = []\n","        current_entry = {'speaker': None, 'start_time': None, 'text': [], 'end_time': None}\n","        for segment in transcript_data['segments']:\n","            start_time = segment['start']; end_time = segment['end']\n","            raw_text = (segment['text'] or \"\").strip()\n","            if not raw_text:\n","                continue\n","            current_speaker = find_speaker_at_time(start_time)\n","\n","            start_new = False\n","            if not current_entry['speaker']:\n","                start_new = True\n","            elif current_entry['speaker'] != current_speaker:\n","                start_new = True\n","            elif current_entry['end_time'] is not None and start_time - current_entry['end_time'] > 2:\n","                start_new = True\n","\n","            if start_new:\n","                if current_entry['speaker']:\n","                    compiled_entries.append(current_entry)\n","                current_entry = {\n","                    'speaker': current_speaker,\n","                    'start_time': start_time,\n","                    'text': [raw_text],\n","                    'end_time': end_time\n","                }\n","            else:\n","                current_entry['text'].append(raw_text)\n","                current_entry['end_time'] = end_time\n","        if current_entry['speaker']:\n","            compiled_entries.append(current_entry)\n","\n","        # Styles\n","        styles = getSampleStyleSheet()\n","        contribution_style = ParagraphStyle('ContributionStyle', parent=styles['Normal'],\n","                                            fontName='Helvetica', fontSize=10, leading=12, wordWrap='CJK')\n","        header_style = ParagraphStyle('HeaderStyle', parent=styles['Normal'], fontName='Helvetica-Bold',\n","                                      fontSize=12, textColor=colors.whitesmoke, alignment=1)\n","        speaker_style = ParagraphStyle('SpeakerStyle', parent=styles['Normal'],\n","                                       fontName='Helvetica', fontSize=10, leading=12, wordWrap='CJK')\n","\n","        # PDF\n","        suffix = \"names\" if privacy_mode == \"names\" else \"ids\"\n","        output_path = f\"/content/session_{class_id}_transcript_{suffix}.pdf\"\n","        doc = SimpleDocTemplate(output_path, pagesize=letter, rightMargin=60, leftMargin=60, topMargin=60, bottomMargin=60)\n","\n","        elements = []\n","\n","        # ====== HEADER / TITLE ======\n","        session_line = class_meta.get('session_title') or f\"Session {class_id}\"\n","        elements.append(Paragraph(session_line, styles['Title']))\n","\n","        # Centered schedule line (sec_sched)\n","        sec_sched = class_meta.get('section_title', '') or class_meta.get('schedule', '')\n","        if sec_sched:\n","            centered_info_style = ParagraphStyle('CenteredInfo', parent=styles['Heading3'], alignment=1)\n","            elements.append(Paragraph(sec_sched, centered_info_style))\n","        elements.append(Spacer(1, 10))\n","\n","        # Left-aligned meta lines (bold labels)\n","        # Class ID, Class Date/Time from recording_start (UTC), Class Link\n","        rec_start = class_meta.get('recording_start')\n","        dt_str = \"\"\n","        if rec_start:\n","            try:\n","                dt = iso8601.parse_date(rec_start).astimezone(timezone.utc)\n","                dt_str = dt.strftime(\"%Y-%m-%d %H:%M UTC\")\n","            except:\n","                dt_str = _safe_date(rec_start)\n","        meta_lines = []\n","        meta_lines.append(Paragraph(f\"<b>Class ID:</b> {class_meta.get('class_id','')}\", styles['Normal']))\n","        if dt_str:\n","            meta_lines.append(Paragraph(f\"<b>Class Date/Time:</b> {dt_str}\", styles['Normal']))\n","        link = class_meta.get('class_link') or \"\"\n","        if link:\n","            meta_lines.append(Paragraph(f\"<b>Class Link:</b> {link}\", styles['Normal']))\n","        for p in meta_lines:\n","            elements.append(p)\n","        elements.append(Spacer(1, 14))\n","\n","        # ====== ATTENDANCE TABLE ======\n","        if attendance:\n","            elements.append(Paragraph(\"Attendance\", styles['Heading3']))\n","            att_rows = [[Paragraph('Student', header_style), Paragraph('Status', header_style)]]\n","            for a in attendance:\n","                display_name = a.get('name','')\n","                if privacy_mode == \"ids\" and a.get('id'):\n","                    display_name = f\"ID {a['id']}\"\n","                att_rows.append([\n","                    Paragraph(soft_break_long_token(display_name, 14), speaker_style),\n","                    Paragraph(\"Absent\" if a.get('absent') else \"Present\", speaker_style)\n","                ])\n","            att_table = Table(att_rows, colWidths=[4.5*inch, 1.5*inch], repeatRows=1)\n","            att_style = TableStyle([\n","                ('BACKGROUND', (0,0), (-1,0), colors.grey),\n","                ('TEXTCOLOR', (0,0), (-1,0), colors.whitesmoke),\n","                ('ALIGN', (0,0), (-1,0), 'CENTER'),\n","                ('FONTNAME', (0,0), (-1,0), 'Helvetica-Bold'),\n","                ('FONTNAME', (0,1), (-1,-1), 'Helvetica'),\n","                ('FONTSIZE', (0,1), (-1,-1), 10),\n","                ('VALIGN', (0,0), (-1,-1), 'TOP'),\n","                ('GRID', (0,0), (-1,-1), 1, colors.black),\n","                ('LEFTPADDING', (0,0), (-1,-1), 6),\n","                ('RIGHTPADDING', (0,0), (-1,-1), 6),\n","                ('TOPPADDING', (0,0), (-1,-1), 3),\n","                ('BOTTOMPADDING', (0,0), (-1,-1), 3),\n","            ])\n","            for i, a in enumerate(attendance, start=1):\n","                color = colors.red if a.get('absent') else colors.green\n","                att_style.add('TEXTCOLOR', (1,i), (1,i), color)\n","            att_table.setStyle(att_style)\n","            elements.append(att_table)\n","            elements.append(Spacer(1, 16))\n","\n","        # ====== CLASS EVENTS TABLE ======\n","        if timeline_segments:\n","            elements.append(Paragraph(\"Class Events\", styles['Heading3']))\n","            events_data_rows = [[Paragraph('Time', header_style),\n","                                 Paragraph('Section', header_style),\n","                                 Paragraph('Event', header_style)]]\n","            for seg in timeline_segments:\n","                events_data_rows.append([\n","                    _fmt_mmss(seg.get('offset_seconds')),\n","                    Paragraph((seg.get('section','') or ''), styles['Normal']),\n","                    Paragraph((seg.get('title','') or ''), styles['Normal']),\n","                ])\n","            # Adjusted widths to reduce overlap, text wrapped via Paragraph\n","            events_table = Table(events_data_rows, colWidths=[0.9*inch, 2.2*inch, 3.9*inch], repeatRows=1)\n","            events_table.setStyle(TableStyle([\n","                ('BACKGROUND', (0,0), (-1,0), colors.grey),\n","                ('TEXTCOLOR', (0,0), (-1,0), colors.whitesmoke),\n","                ('ALIGN', (0,0), (-1,0), 'CENTER'),\n","                ('FONTNAME', (0,0), (-1,0), 'Helvetica-Bold'),\n","                ('FONTNAME', (0,1), (-1,-1), 'Helvetica'),\n","                ('FONTSIZE', (0,1), (-1,-1), 10),\n","                ('VALIGN', (0,0), (-1,-1), 'TOP'),\n","                ('GRID', (0,0), (-1,-1), 1, colors.black),\n","                ('LEFTPADDING', (0,0), (-1,-1), 6),\n","                ('RIGHTPADDING', (0,0), (-1,-1), 6),\n","                ('TOPPADDING', (0,0), (-1,-1), 3),\n","                ('BOTTOMPADDING', (0,0), (-1,-1), 3),\n","            ]))\n","            elements.append(events_table)\n","            elements.append(Spacer(1, 16))\n","\n","        # ====== TRANSCRIPT (break only by class events) ======\n","        elements.append(Paragraph(\"Transcript\", styles['Heading3']))\n","        elements.append(Spacer(1, 6))\n","\n","        # Flatten and normalize\n","        all_items = []\n","        for entry in compiled_entries:\n","            text = normalize_sentence_spacing(' '.join(entry['text']).strip())\n","            if text in ['...', '.', '', 'Mm-hmm.'] or len(text) < 3:\n","                continue\n","            timestamp = _fmt_mmss(entry['start_time'])\n","            # split into printable chunks to avoid super long cells\n","            max_chars_per_chunk = 500\n","            sentences = re.split(r'(?<=[.!?])\\s+', text)\n","            chunks, curr = [], \"\"\n","            for s in sentences:\n","                candidate = (curr + s + \" \").strip() if curr else (s + \" \")\n","                if len(candidate) <= max_chars_per_chunk:\n","                    curr = candidate\n","                else:\n","                    if curr: chunks.append(curr.strip())\n","                    curr = s + \" \"\n","            if curr: chunks.append(curr.strip())\n","\n","            for i, chunk in enumerate(chunks or [text]):\n","                display_ts = \"(cont.)\" if i > 0 else timestamp\n","                all_items.append({\n","                    'start_time': entry['start_time'],\n","                    'end_time': entry['end_time'],\n","                    'timestamp': display_ts,\n","                    'speaker': entry['speaker'],\n","                    'text': chunk\n","                })\n","        all_items.sort(key=lambda x: x['start_time'])\n","\n","        # Build event windows; include preamble\n","        seg_windows = []\n","        if timeline_segments:\n","            first_start = max(0, (timeline_segments[0].get('offset_seconds') or 0))\n","            if first_start > 0:\n","                seg_windows.append({'start': 0, 'end': first_start, 'label': f\"{_fmt_mmss(0)} — Before first event\"})\n","            for idx, seg in enumerate(timeline_segments):\n","                start = max(0, (seg.get('offset_seconds') or 0))\n","                end = (timeline_segments[idx+1].get('offset_seconds') if idx+1 < len(timeline_segments) else float('inf')) or float('inf')\n","                bits = []\n","                if seg.get('section'): bits.append(seg['section'])\n","                if seg.get('title'):   bits.append(seg['title'])\n","                label = f\"{_fmt_mmss(start)} — \" + (' · '.join(bits) if bits else 'Event')\n","                seg_windows.append({'start': start, 'end': end, 'label': label})\n","        else:\n","            seg_windows.append({'start': 0, 'end': float('inf'), 'label': \"Transcript\"})\n","\n","        for win in seg_windows:\n","            bucket = [it for it in all_items if win['start'] <= it['start_time'] < win['end']]\n","            if not bucket:\n","                continue\n","            elements.append(Paragraph(win['label'], styles['Heading4']))\n","            elements.append(Spacer(1, 4))\n","\n","            data = [[Paragraph('Time', header_style),\n","                     Paragraph('Speaker', header_style),\n","                     Paragraph('Contribution', header_style)]]\n","            for item in bucket:\n","                spk_txt = soft_break_long_token(item['speaker'], 14)\n","                data.append([\n","                    item['timestamp'],\n","                    Paragraph(spk_txt, speaker_style),\n","                    Paragraph(normalize_sentence_spacing(item['text']), contribution_style)\n","                ])\n","\n","            table = Table(data, colWidths=[0.75*inch, 2.1*inch, 4.25*inch], repeatRows=1)\n","            table.setStyle(TableStyle([\n","                ('BACKGROUND', (0, 0), (-1, 0), colors.grey),\n","                ('TEXTCOLOR', (0, 0), (-1, 0), colors.whitesmoke),\n","                ('ALIGN', (0, 0), (-1, 0), 'CENTER'),\n","                ('FONTNAME', (0, 0), (-1, 0), 'Helvetica-Bold'),\n","                ('FONTNAME', (0, 1), (-1, -1), 'Helvetica'),\n","                ('FONTSIZE', (0, 1), (-1, -1), 10),\n","                ('VALIGN', (0, 0), (-1, -1), 'TOP'),\n","                ('GRID', (0, 0), (-1, -1), 1, colors.black),\n","                ('LEFTPADDING', (0, 0), (-1, -1), 6),\n","                ('RIGHTPADDING', (0, 0), (-1, -1), 6),\n","                ('TOPPADDING', (0, 0), (-1, -1), 3),\n","                ('BOTTOMPADDING', (0, 0), (-1, -1), 3),\n","            ]))\n","            elements.append(table)\n","            elements.append(Spacer(1, 12))\n","\n","        doc.build(elements)\n","        print(f\"Created PDF transcript: {output_path}\")\n","        return output_path\n","\n","    except Exception as e:\n","        print(f\"Error processing transcript: {str(e)}\")\n","        raise e\n","\n","def compile_transcript_to_csv(class_id, headers, privacy_mode=\"names\"):\n","    \"\"\"\n","    CSV:\n","      - Header block (centered items as lines)\n","      - Attendance\n","      - Class Events\n","      - Transcript with event headers\n","    \"\"\"\n","    try:\n","        with open(f\"/content/session_{class_id}_transcript.json\", 'r') as f:\n","            transcript_data = json.load(f)\n","        with open(f\"/content/session_{class_id}_events.json\", 'r') as f:\n","            events_data = json.load(f)\n","\n","        class_meta = events_data.get('class_meta', {})\n","        timeline_segments = events_data.get('timeline_segments', [])\n","        attendance = events_data.get('attendance', [])\n","\n","        # Speaker mapping\n","        speaker_map = _build_speaker_window_map(events_data, privacy_mode)\n","\n","        def find_speaker_at_time(t):\n","            for (start, end), spk in speaker_map.items():\n","                if start <= t <= end:\n","                    return spk\n","            return \"Professor\"\n","\n","        # Combine segments by speaker\n","        compiled_entries = []\n","        current = {'speaker': None, 'start_time': None, 'text': [], 'end_time': None}\n","        for seg in transcript_data['segments']:\n","            st, en, tx = seg['start'], seg['end'], (seg['text'] or '').strip()\n","            if not tx:\n","                continue\n","            spk = find_speaker_at_time(st)\n","\n","            start_new = (not current['speaker']) or (current['speaker'] != spk) or (current['end_time'] is not None and st - current['end_time'] > 2)\n","            if start_new:\n","                if current['speaker']:\n","                    compiled_entries.append(current)\n","                current = {'speaker': spk, 'start_time': st, 'text': [tx], 'end_time': en}\n","            else:\n","                current['text'].append(tx)\n","                current['end_time'] = en\n","        if current['speaker']:\n","            compiled_entries.append(current)\n","\n","        # Flatten\n","        all_items = []\n","        for entry in compiled_entries:\n","            text = normalize_sentence_spacing(' '.join(entry['text']).strip())\n","            if text in ['...', '.', '', 'Mm-hmm.'] or len(text) < 3:\n","                continue\n","            timestamp = _fmt_mmss(entry['start_time'])\n","            all_items.append({\n","                'timestamp': timestamp,\n","                'speaker': entry['speaker'],\n","                'text': text,\n","                'start_time': entry['start_time'],\n","                'end_time': entry['end_time']\n","            })\n","        all_items.sort(key=lambda x: x['start_time'])\n","\n","        # Build event windows\n","        segmented_rows = []\n","        seg_windows = []\n","        if timeline_segments:\n","            first_start = max(0, (timeline_segments[0].get('offset_seconds') or 0))\n","            if first_start > 0:\n","                seg_windows.append({'start': 0, 'end': first_start, 'label': f\"{_fmt_mmss(0)} — Before first event\"})\n","            for idx, seg in enumerate(timeline_segments):\n","                start = max(0, (seg.get('offset_seconds') or 0))\n","                end = (timeline_segments[idx+1].get('offset_seconds') if idx+1 < len(timeline_segments) else float('inf')) or float('inf')\n","                bits = []\n","                if seg.get('section'): bits.append(seg['section'])\n","                if seg.get('title'):   bits.append(seg['title'])\n","                label = f\"{_fmt_mmss(start)} — \" + (' / '.join(bits) if bits else 'Event')\n","                seg_windows.append({'start': start, 'end': end, 'label': label})\n","        else:\n","            seg_windows.append({'start': 0, 'end': float('inf'), 'label': \"Transcript\"})\n","\n","        for win in seg_windows:\n","            bucket = [it for it in all_items if win['start'] <= it['start_time'] < win['end']]\n","            if not bucket:\n","                continue\n","            segmented_rows.append({'timestamp': '', 'speaker': '', 'text': f\"--- {win['label']} ---\"})\n","            segmented_rows.extend(bucket)\n","\n","        out_rows = segmented_rows or all_items\n","\n","        # Write CSV\n","        suffix = \"names\" if privacy_mode == \"names\" else \"ids\"\n","        output_path = f\"/content/session_{class_id}_transcript_{suffix}.csv\"\n","        with open(output_path, 'w', newline='', encoding='utf-8') as csvfile:\n","            w = csv.writer(csvfile)\n","\n","            # Header block\n","            w.writerow([\"Session\", class_meta.get('session_title','')])\n","            sec_sched = class_meta.get('section_title') or class_meta.get('schedule') or ''\n","            if sec_sched:\n","                w.writerow([sec_sched])\n","            # Bold not supported in CSV, but include meta lines:\n","            w.writerow([\"Class ID\", class_meta.get('class_id','')])\n","            rec_start = class_meta.get('recording_start')\n","            dt_str = \"\"\n","            if rec_start:\n","                try:\n","                    dt = iso8601.parse_date(rec_start).astimezone(timezone.utc)\n","                    dt_str = dt.strftime(\"%Y-%m-%d %H:%M UTC\")\n","                except:\n","                    dt_str = _safe_date(rec_start)\n","            if dt_str:\n","                w.writerow([\"Class Date/Time\", dt_str])\n","            link = class_meta.get('class_link') or \"\"\n","            if link:\n","                w.writerow([\"Class Link\", link])\n","            w.writerow([])\n","\n","            # Attendance\n","            if attendance:\n","                w.writerow([\"Attendance\"])\n","                w.writerow([\"Student\", \"Status\"])\n","                for a in attendance:\n","                    display_name = a.get('name','')\n","                    if privacy_mode == \"ids\" and a.get('id'):\n","                        display_name = f\"ID {a['id']}\"\n","                    w.writerow([display_name, \"Absent\" if a.get('absent') else \"Present\"])\n","                w.writerow([])\n","\n","            # Class Events\n","            if timeline_segments:\n","                w.writerow([\"Class Events\"])\n","                w.writerow([\"Time\", \"Section\", \"Event\"])\n","                for seg in timeline_segments:\n","                    w.writerow([_fmt_mmss(seg.get('offset_seconds')), seg.get('section',''), seg.get('title','')])\n","                w.writerow([])\n","\n","            # Transcript\n","            w.writerow(['Time', 'Speaker', 'Contribution'])\n","            for row in out_rows:\n","                w.writerow([row.get('timestamp',''), row.get('speaker',''), row.get('text','')])\n","\n","        print(f\"Created CSV transcript: {output_path}\")\n","        return output_path\n","\n","    except Exception as e:\n","        print(f\"Error creating CSV transcript: {str(e)}\")\n","        return None\n","\n","def create_simplified_csv(class_id, transcript_path):\n","    \"\"\"\n","    Fallback CSV: just time + text.\n","    \"\"\"\n","    try:\n","        with open(f\"/content/session_{class_id}_transcript.json\", 'r') as f:\n","            transcript_data = json.load(f)\n","\n","        output_path = f\"/content/session_{class_id}_transcript_simple.csv\"\n","        with open(output_path, 'w', newline='', encoding='utf-8') as csvfile:\n","            w = csv.writer(csvfile)\n","            w.writerow(['Time', 'Text'])\n","            for seg in transcript_data['segments']:\n","                minutes = int(seg['start'] // 60)\n","                seconds = int(seg['start'] % 60)\n","                timestamp = f\"{minutes:02d}:{seconds:02d}\"\n","                w.writerow([timestamp, normalize_sentence_spacing(seg['text'])])\n","\n","        print(f\"Created simplified CSV transcript: {output_path}\")\n","        return output_path\n","    except Exception as e:\n","        print(f\"Error creating simplified CSV transcript: {str(e)}\")\n","        return None\n","\n","def create_simplified_transcript(class_id, transcript_path):\n","    \"\"\"\n","    Fallback PDF: no events, no speakers; includes minimal title.\n","    \"\"\"\n","    try:\n","        output_path = f\"/content/session_{class_id}_transcript_simple.pdf\"\n","        styles = getSampleStyleSheet()\n","        text_style = ParagraphStyle('TextStyle', parent=styles['Normal'],\n","                                    fontName='Helvetica', fontSize=10, leading=12, spaceAfter=0, spaceBefore=0,\n","                                    wordWrap='CJK')\n","        header_style = ParagraphStyle('HeaderStyle', parent=styles['Normal'],\n","                                      fontName='Helvetica-Bold', fontSize=12, textColor=colors.whitesmoke,\n","                                      alignment=1)\n","\n","        with open(f\"/content/session_{class_id}_transcript.json\", 'r') as f:\n","            transcript_data = json.load(f)\n","\n","        doc = SimpleDocTemplate(output_path, pagesize=letter,\n","                                rightMargin=72, leftMargin=72, topMargin=72, bottomMargin=72)\n","        elements = []\n","        title = Paragraph(f\"Session {class_id}\", styles['Title'])\n","        date_str = datetime.datetime.now().strftime(\"%Y-%m-%d\")\n","        subtitle = Paragraph(f\"Generated on {date_str}\", styles['Heading2'])\n","        elements.append(title); elements.append(subtitle); elements.append(Spacer(1, 12))\n","\n","        data = [[Paragraph('Time', header_style), Paragraph('Text', header_style)]]\n","        for seg in transcript_data['segments']:\n","            minutes = int(seg['start'] // 60)\n","            seconds = int(seg['start'] % 60)\n","            timestamp = f\"{minutes:02d}:{seconds:02d}\"\n","            data.append([timestamp, Paragraph(normalize_sentence_spacing(seg['text']), text_style)])\n","\n","        table = Table(data, colWidths=[0.75*inch, 6.25*inch], repeatRows=1)\n","        table.setStyle(TableStyle([\n","            ('BACKGROUND', (0, 0), (-1, 0), colors.grey),\n","            ('TEXTCOLOR', (0, 0), (-1, 0), colors.whitesmoke),\n","            ('ALIGN', (0, 0), (-1, 0), 'CENTER'),\n","            ('FONTNAME', (0, 0), (-1, 0), 'Helvetica-Bold'),\n","            ('FONTNAME', (0, 1), (-1, -1), 'Helvetica'),\n","            ('FONTSIZE', (0, 1), (-1, -1), 10),\n","            ('VALIGN', (0, 0), (-1, -1), 'TOP'),\n","            ('GRID', (0, 0), (-1, -1), 1, colors.black),\n","            ('LEFTPADDING', (0, 0), (-1, -1), 6),\n","            ('RIGHTPADDING', (0, 0), (-1, -1), 6),\n","            ('TOPPADDING', (0, 0), (-1, -1), 3),\n","            ('BOTTOMPADDING', (0, 0), (-1, -1), 3),\n","        ]))\n","        elements.append(table)\n","        doc.build(elements)\n","        print(f\"Created simplified PDF transcript: {output_path}\")\n","        return output_path\n","    except Exception as e:\n","        print(f\"Error creating simplified transcript: {str(e)}\")\n","        return None"]},{"cell_type":"code","source":["# ===========================\n","# Main pipeline\n","# ===========================\n","\n","# 1) cURL\n","print(\"1) Paste your Forum cURL (right-click in Chrome DevTools → Copy as cURL)\")\n","raw_curl = input(\"cURL: \").strip()\n","clear_output()\n","\n","# Auto-derive Class ID from the cURL (uses your existing helper)\n","try:\n","    _ids = extract_ids_from_curl(raw_curl)\n","except NameError:\n","    _ids = {}\n","CLASS_ID = _ids.get(\"class_id\")\n","\n","# 2) Media path or URL\n","print(\"2) Provide your media file\")\n","print(\"   • Enter a local/Drive path like /content/your_file.mp3\")\n","print(\"   • OR paste a https:// URL to the media file (mp3/mp4/wav)\")\n","AUDIO_PATH = input(\"Path or URL: \").strip()\n","clear_output()\n","\n","# 3) Privacy mode (with a real input box + validation loop)\n","print(\"3) Student name privacy mode\")\n","print(\"   Type one of: names  (show names)  |  ids  (anonymize to IDs)  |  both  (generate both)\")\n","while True:\n","    PRIVACY_MODE = input(\"Privacy mode [names/ids/both]: \").strip().lower()\n","    if PRIVACY_MODE in (\"names\", \"ids\", \"both\"):\n","        break\n","    print(\"Please type exactly: names, ids, or both.\")\n","\n","clear_output()\n","\n","# (Optional) custom terms to preserve spellings\n","print(\"4) Optional: comma-separated custom terms to preserve spellings (press Enter to skip)\")\n","USER_TERMS_RAW = input(\"Custom terms: \").strip()\n","USER_TERMS = [t.strip() for t in USER_TERMS_RAW.split(\",\") if t.strip()]\n","clear_output()\n","\n","# Summary\n","print(\"Thanks! Summary:\\n\")\n","print(f\"Class ID (detected): {CLASS_ID or '(will fetch from API if needed)'}\")\n","print(f\"Media: {AUDIO_PATH}\")\n","print(f\"Privacy mode: {PRIVACY_MODE}\")\n","if USER_TERMS:\n","    print(f\"Custom terms: {USER_TERMS}\")\n","print(\"\\nStarting the transcript generation process…\")\n","print(\"⏳ This can take a while depending on file length and model size...\")\n","\n","def process_lecture(audio_path, class_id, curl_string, privacy_mode=\"names\"):\n","    try:\n","        print(\"Step 1/4: Fetching Forum class events...\")\n","        headers = clean_curl(curl_string)\n","        events_data = get_forum_events(class_id, headers, curl_string)\n","\n","        print(\"\\nStep 2/4: Preprocessing audio...\")\n","        preprocessor = AudioPreprocessor()\n","        fixed_path = preprocessor.validate_and_fix_file(audio_path)\n","\n","        print(\"\\nStep 3/4: Transcribing...\")\n","        tp = TranscriptionProcessor()\n","        transcript_path = tp.transcribe(fixed_path, class_id)\n","\n","        print(\"\\nStep 4/4: Preparing outputs...\")\n","\n","        # Choose output modes\n","        modes = [privacy_mode] if privacy_mode in (\"names\", \"ids\") else [\"names\", \"ids\"]\n","\n","        outputs = []\n","        for mode in modes:\n","            pdf_path = compile_transcript_to_pdf(class_id, headers, privacy_mode=mode)\n","            csv_path = compile_transcript_to_csv(class_id, headers, privacy_mode=mode)\n","            outputs.append((mode, pdf_path, csv_path))\n","\n","        # Accuracy caution\n","        print(\"\\n⚠️  Accuracy caution: Do not rely solely on this transcript. Manually verify key information.\")\n","\n","        # Clean up temp WAV (but keep original)\n","        try:\n","            if fixed_path and Path(fixed_path).exists() and (str(fixed_path) != str(audio_path)):\n","                Path(fixed_path).unlink()\n","                print(f\"Cleaned up temporary file: {fixed_path}\")\n","        except Exception as cleanup_error:\n","            print(f\"Note: Could not clean up temporary files: {str(cleanup_error)}\")\n","\n","        return outputs\n","\n","    except Exception as e:\n","        print(f\"\\nERROR: {str(e)}\")\n","        if \"MP4\" in str(e) and audio_path.lower().endswith('.mp4'):\n","            print(\"\\nThere was a problem with your MP4 file. Suggestions:\")\n","            print(\"1. Try converting it to MP3 or WAV locally before uploading\")\n","            print(\"2. Use a screen recorder to re-record the audio\")\n","            print(\"3. Contact Forum support about MP4 download issues\")\n","        else:\n","            print(\"\\nTranscription failed. Please try again with a different file or path.\")\n","        return []\n","\n","# Run\n","outs = process_lecture(AUDIO_PATH, CLASS_ID, raw_curl, PRIVACY_MODE)\n","\n","# CUDA cleanup\n","try:\n","    torch.cuda.empty_cache()\n","except:\n","    pass\n","import gc; gc.collect()\n","print(\"CUDA cache cleared\")\n","\n","# Pretty print results\n","if outs:\n","    if len(outs) == 1:\n","        mode, pdfp, csvp = outs[0]\n","        print(f\"\\nSuccess! Your transcripts are ready ({mode}):\")\n","        print(f\"PDF: {pdfp}\")\n","        print(f\"CSV: {csvp}\")\n","    else:\n","        # both\n","        print(\"\\nSuccess! Your transcripts are ready (both privacy modes):\")\n","        for mode, pdfp, csvp in outs:\n","            print(f\"PDF ({mode}): {pdfp}\")\n","            print(f\"CSV ({mode}): {csvp}\")"],"metadata":{"id":"GifN92Arp2FJ","colab":{"base_uri":"https://localhost:8080/","height":842,"referenced_widgets":["06b9bf52499b4651a8a8c1f9ae6fc84e","4c8319fde607431592c7634e847cd347","4e66d5bbae8547d5a57afb4d473332f0","10df34c71d864d5cbe92d53364aea36e","1012e1bf20454845a3829c047e1f5fd7","7e7c1ce8491d401496afec4013536540","12fd8c1fde6c44b59d64c5a676959244","87c16581550c413c8bbce7b0479a60bc","0143542d5c3e4fd9a0c321e508e278b2","cf3a18279e4c4f3aa6a192306f498506","f624eb9c3f2b44bda6b73d1791cbf0bf"]},"executionInfo":{"status":"error","timestamp":1755384311139,"user_tz":180,"elapsed":194771,"user":{"displayName":"Aleksei Korablev","userId":"02837062072924939756"}},"outputId":"2cdbed62-8a86-489f-9399-f9d2d14d92c4"},"execution_count":16,"outputs":[{"output_type":"stream","name":"stdout","text":["Thanks! Summary:\n","\n","Class ID (detected): 87534\n","Media: https://d2r3twttwe1v7v.cloudfront.net/6b061a86-d83e-4cbe-8c29-4775affe50ac/zencoder-recording-600k.mp4?Expires=1755391364&Signature=baFisiJ86laVhhz7rBNBbwnYhF4hDhZjr5cCMpYnPjSl29-Jzza8JRD9TUaCwuzirjtLtx3D2Q7ucPv9n0eqSz1XeJMnzBXq2G5r8dy31JW1NZwzT98PkC2yYnEujS7c-5m1IM1NTTtB~kHPJl0g~v0mKRIc2Cc75HFjKcr-DzR-QUXNpvsCtyfF17bHXAj0WBG6UlW5vPEDUymDnTxuYQX7p6BdnQI7pNEXfAq6vatwoP9OwAhVOluy3gENyBRxgYqW7hN63HfeQBvF1erVuEevGqSGOG~unQNNBOwqSOZaufSLznW6oAnw0OJSyVWXH96a7lyId8EZh0-xSkCVLw__&Key-Pair-Id=K2PZFX9H2TL8YQ\n","Privacy mode: both\n","\n","Starting the transcript generation process…\n","⏳ This can take a while depending on file length and model size...\n","Step 1/4: Fetching Forum class events...\n","Fetching class and event data from Forum...\n","Processed voice events: 160; timeline segments: 7; attendance: 23\n","\n","Step 2/4: Preprocessing audio...\n","Validating file or URL: https://d2r3twttwe1v7v.cloudfront.net/6b061a86-d83e-4cbe-8c29-4775affe50ac/zencoder-recording-600k.mp4?Expires=1755391364&Signature=baFisiJ86laVhhz7rBNBbwnYhF4hDhZjr5cCMpYnPjSl29-Jzza8JRD9TUaCwuzirjtLtx3D2Q7ucPv9n0eqSz1XeJMnzBXq2G5r8dy31JW1NZwzT98PkC2yYnEujS7c-5m1IM1NTTtB~kHPJl0g~v0mKRIc2Cc75HFjKcr-DzR-QUXNpvsCtyfF17bHXAj0WBG6UlW5vPEDUymDnTxuYQX7p6BdnQI7pNEXfAq6vatwoP9OwAhVOluy3gENyBRxgYqW7hN63HfeQBvF1erVuEevGqSGOG~unQNNBOwqSOZaufSLznW6oAnw0OJSyVWXH96a7lyId8EZh0-xSkCVLw__&Key-Pair-Id=K2PZFX9H2TL8YQ\n","Detected URL — downloading...\n","Downloaded to: /content/input_downloaded.mp4\n","Converting MP4 → Whisper-optimized WAV...\n","Created: /content/input_downloaded.wav\n","\n","Step 3/4: Transcribing...\n","Using device: cuda\n","Loading Whisper model...\n"]},{"output_type":"stream","name":"stderr","text":["100%|█████████████████████████████████████| 1.42G/1.42G [00:21<00:00, 69.7MiB/s]\n"]},{"output_type":"stream","name":"stdout","text":["Processing audio to generate transcript JSON...\n","Total duration: 1:41:00\n"]},{"output_type":"display_data","data":{"text/plain":["Processing segments:   0%|          | 0/1 [00:00<?, ?segment/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"06b9bf52499b4651a8a8c1f9ae6fc84e"}},"metadata":{}},{"output_type":"error","ename":"KeyboardInterrupt","evalue":"","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m/tmp/ipython-input-38633953.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     99\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    100\u001b[0m \u001b[0;31m# Run\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 101\u001b[0;31m \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprocess_lecture\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mAUDIO_PATH\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mCLASS_ID\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mraw_curl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mPRIVACY_MODE\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    102\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    103\u001b[0m \u001b[0;31m# CUDA cleanup\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/tmp/ipython-input-38633953.py\u001b[0m in \u001b[0;36mprocess_lecture\u001b[0;34m(audio_path, class_id, curl_string, privacy_mode)\u001b[0m\n\u001b[1;32m     61\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"\\nStep 3/4: Transcribing...\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m         \u001b[0mtp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTranscriptionProcessor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m         \u001b[0mtranscript_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranscribe\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfixed_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclass_id\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"\\nStep 4/4: Preparing outputs...\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/tmp/ipython-input-2636347790.py\u001b[0m in \u001b[0;36mtranscribe\u001b[0;34m(self, audio_path, class_id)\u001b[0m\n\u001b[1;32m     47\u001b[0m                 \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m                     \u001b[0;32mwith\u001b[0m \u001b[0mctx_mgr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 49\u001b[0;31m                         result = self.model.transcribe(\n\u001b[0m\u001b[1;32m     50\u001b[0m                             \u001b[0mtemp_path\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m                             \u001b[0mword_timestamps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/whisper/transcribe.py\u001b[0m in \u001b[0;36mtranscribe\u001b[0;34m(model, audio, verbose, temperature, compression_ratio_threshold, logprob_threshold, no_speech_threshold, condition_on_previous_text, initial_prompt, carry_initial_prompt, word_timestamps, prepend_punctuations, append_punctuations, clip_timestamps, hallucination_silence_threshold, **decode_options)\u001b[0m\n\u001b[1;32m    293\u001b[0m                 \u001b[0mdecode_options\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"prompt\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mall_tokens\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mprompt_reset_since\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    294\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 295\u001b[0;31m             \u001b[0mresult\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mDecodingResult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdecode_with_fallback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmel_segment\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    296\u001b[0m             \u001b[0mtokens\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtokens\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    297\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/whisper/transcribe.py\u001b[0m in \u001b[0;36mdecode_with_fallback\u001b[0;34m(segment)\u001b[0m\n\u001b[1;32m    199\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    200\u001b[0m             \u001b[0moptions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDecodingOptions\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtemperature\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 201\u001b[0;31m             \u001b[0mdecode_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msegment\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    202\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    203\u001b[0m             \u001b[0mneeds_fallback\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/utils/_contextlib.py\u001b[0m in \u001b[0;36mdecorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    114\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    115\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mctx_factory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 116\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    117\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    118\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/whisper/decoding.py\u001b[0m in \u001b[0;36mdecode\u001b[0;34m(model, mel, options, **kwargs)\u001b[0m\n\u001b[1;32m    822\u001b[0m         \u001b[0moptions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mreplace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    823\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 824\u001b[0;31m     \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDecodingTask\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    825\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    826\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0msingle\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/utils/_contextlib.py\u001b[0m in \u001b[0;36mdecorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    114\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    115\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mctx_factory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 116\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    117\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    118\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/whisper/decoding.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, mel)\u001b[0m\n\u001b[1;32m    735\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    736\u001b[0m         \u001b[0;31m# call the main sampling loop\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 737\u001b[0;31m         \u001b[0mtokens\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msum_logprobs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mno_speech_probs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_main_loop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maudio_features\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtokens\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    738\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    739\u001b[0m         \u001b[0;31m# reshape the tensors to have (n_audio, n_group) as the first two dimensions\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/whisper/decoding.py\u001b[0m in \u001b[0;36m_main_loop\u001b[0;34m(self, audio_features, tokens)\u001b[0m\n\u001b[1;32m    685\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    686\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msample_len\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 687\u001b[0;31m                 \u001b[0mlogits\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minference\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlogits\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtokens\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maudio_features\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    688\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    689\u001b[0m                 if (\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/whisper/decoding.py\u001b[0m in \u001b[0;36mlogits\u001b[0;34m(self, tokens, audio_features)\u001b[0m\n\u001b[1;32m    161\u001b[0m             \u001b[0mtokens\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtokens\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    162\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 163\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtokens\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maudio_features\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkv_cache\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkv_cache\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    164\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    165\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mcleanup_caching\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1739\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1740\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1741\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1748\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1751\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1752\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/whisper/model.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x, xa, kv_cache)\u001b[0m\n\u001b[1;32m    240\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    241\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mblock\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mblocks\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 242\u001b[0;31m             \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mblock\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mxa\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkv_cache\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mkv_cache\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    243\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    244\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mln\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1739\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1740\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1741\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1741\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1742\u001b[0m     \u001b[0;31m# fmt: off\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1743\u001b[0;31m     \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1744\u001b[0m         \u001b[0mforward_call\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_tracing_state\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1745\u001b[0m         \u001b[0;31m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "]}]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Dbtt5NLEYHvL","executionInfo":{"status":"aborted","timestamp":1755384311142,"user_tz":180,"elapsed":206283,"user":{"displayName":"Aleksei Korablev","userId":"02837062072924939756"}}},"outputs":[],"source":["# @title\n","def process_lecture(audio_path, class_id, curl_string):\n","    \"\"\"\n","    End-to-end pipeline with graceful fallbacks.\n","    \"\"\"\n","    output_pdf, output_csv = None, None  # NEW: ensure defined for the return\n","    try:\n","        # 1) Forum events\n","        print(\"Step 1/4: Processing Forum class events...\")\n","        headers = clean_curl(curl_string)\n","        events_data = get_forum_events(class_id, headers)\n","\n","        # 2) Audio preprocess\n","        print(\"\\nStep 2/4: Preprocessing audio file...\")\n","        # Resolve remote URLs to a local temp file before conversion\n","        resolved_media_path = download_if_url(audio_path, headers)\n","\n","        preprocessor = AudioPreprocessor()\n","        fixed_path = preprocessor.validate_and_fix_file(resolved_media_path)\n","\n","\n","        # 3) Transcribe\n","        print(\"\\nStep 3/4: Generating transcript...\")\n","        tp = TranscriptionProcessor()\n","        transcript_path = tp.transcribe(fixed_path, class_id)\n","\n","        # 4/4: Optional LLM cleanup (Ollama), then compile outputs\n","        print(\"\\nStep 4/4: Preparing outputs...\")\n","\n","        try:\n","            # Build compiled entries once from JSON + Forum events\n","            compiled_entries_raw = build_compiled_entries_from_json(class_id, events_data)\n","\n","            def _emit_variant(entries, variant_label, name_mode, suffix_extra=\"\"):\n","                \"\"\"Emit one PDF/CSV pair for a given entries set + privacy mode.\"\"\"\n","                suffix = f\"_{variant_label}{suffix_extra}\" if variant_label else f\"{suffix_extra}\"\n","                pdf = compile_transcript_to_pdf(\n","                    class_id, headers, name_mode=name_mode, file_suffix=suffix,\n","                    entries_override=entries, output_dir=OUTPUT_DIR\n","                )\n","                csvp = compile_transcript_to_csv(\n","                    class_id, headers, name_mode=name_mode, file_suffix=suffix,\n","                    entries_override=entries, output_dir=OUTPUT_DIR\n","                )\n","                return pdf, csvp\n","\n","            if USE_OLLAMA:\n","                print(\"Running Ollama cleanup (spellings/terms)...\")\n","                corrected_entries = postprocess_with_ollama(\n","                    compiled_entries_raw, events_data, CUSTOM_TERMS,\n","                    model=OLLAMA_MODEL, url=OLLAMA_URL\n","                )\n","\n","                if PRODUCE_BOTH_VARIANTS:\n","                    # Original (orig) + Corrected (corr)\n","                    if PRIVACY_MODE == \"both\":\n","                        on_pdf, on_csv = _emit_variant(compiled_entries_raw, \"orig\", \"names\", \"_names\")\n","                        oi_pdf, oi_csv = _emit_variant(compiled_entries_raw, \"orig\", \"ids\",   \"_ids\")\n","                        cn_pdf, cn_csv = _emit_variant(corrected_entries,    \"corr\", \"names\", \"_names\")\n","                        ci_pdf, ci_csv = _emit_variant(corrected_entries,    \"corr\", \"ids\",   \"_ids\")\n","                        print(\"\\nSuccess! Your transcripts are ready (both variants & both privacy modes):\")\n","                        print(f\"PDF (orig, names): {on_pdf}\"); print(f\"CSV (orig, names): {on_csv}\")\n","                        print(f\"PDF (orig, ids):   {oi_pdf}\"); print(f\"CSV (orig, ids):   {oi_csv}\")\n","                        print(f\"PDF (corr, names): {cn_pdf}\"); print(f\"CSV (corr, names): {cn_csv}\")\n","                        print(f\"PDF (corr, ids):   {ci_pdf}\"); print(f\"CSV (corr, ids):   {ci_csv}\")\n","                        output_pdf, output_csv = cn_pdf, cn_csv\n","                    else:\n","                        variant_suffix = \"\" if PRIVACY_MODE == \"names\" else \"_ids\"\n","                        o_pdf, o_csv = _emit_variant(compiled_entries_raw, \"orig\", PRIVACY_MODE, variant_suffix)\n","                        c_pdf, c_csv = _emit_variant(corrected_entries,    \"corr\", PRIVACY_MODE, variant_suffix)\n","                        print(\"\\nSuccess! Your transcripts are ready (original + corrected):\")\n","                        print(f\"PDF (original):  {o_pdf}\"); print(f\"CSV (original):  {o_csv}\")\n","                        print(f\"PDF (corrected): {c_pdf}\"); print(f\"CSV (corrected): {c_csv}\")\n","                        output_pdf, output_csv = c_pdf, c_csv\n","                else:\n","                    # Corrected only (default)\n","                    if PRIVACY_MODE == \"both\":\n","                        cn_pdf, cn_csv = _emit_variant(corrected_entries, \"corr\", \"names\", \"_names\")\n","                        ci_pdf, ci_csv = _emit_variant(corrected_entries, \"corr\", \"ids\",   \"_ids\")\n","                        print(\"\\nSuccess! Your transcripts are ready (corrected, both privacy modes):\")\n","                        print(f\"PDF (names): {cn_pdf}\"); print(f\"CSV (names): {cn_csv}\")\n","                        print(f\"PDF (ids):   {ci_pdf}\"); print(f\"CSV (ids):   {ci_csv}\")\n","                        output_pdf, output_csv = cn_pdf, cn_csv\n","                    else:\n","                        variant_suffix = \"\" if PRIVACY_MODE == \"names\" else \"_ids\"\n","                        out_pdf, out_csv = _emit_variant(corrected_entries, \"corr\", PRIVACY_MODE, variant_suffix)\n","                        print(\"\\nSuccess! Your transcripts are ready (corrected):\")\n","                        print(f\"PDF: {out_pdf}\"); print(f\"CSV: {out_csv}\")\n","                        output_pdf, output_csv = out_pdf, out_csv\n","            else:\n","                # No Ollama: emit original (raw Whisper) only\n","                if PRIVACY_MODE == \"both\":\n","                    on_pdf, on_csv = _emit_variant(compiled_entries_raw, \"\", \"names\", \"_names\")\n","                    oi_pdf, oi_csv = _emit_variant(compiled_entries_raw, \"\", \"ids\",   \"_ids\")\n","                    print(\"\\nSuccess! Your transcripts are ready (original, both privacy modes):\")\n","                    print(f\"PDF (names): {on_pdf}\"); print(f\"CSV (names): {on_csv}\")\n","                    print(f\"PDF (ids):   {oi_pdf}\"); print(f\"CSV (ids):   {oi_csv}\")\n","                    output_pdf, output_csv = on_pdf, on_csv\n","                else:\n","                    variant_suffix = \"\" if PRIVACY_MODE == \"names\" else \"_ids\"\n","                    out_pdf, out_csv = _emit_variant(compiled_entries_raw, \"\", PRIVACY_MODE, variant_suffix)\n","                    print(\"\\nSuccess! Your transcripts are ready (original):\")\n","                    print(f\"PDF: {out_pdf}\"); print(f\"CSV: {out_csv}\")\n","                    output_pdf, output_csv = out_pdf, out_csv\n","\n","            print(\"\\n⚠️  Accuracy caution: Do not rely solely on this transcript. Manually verify key information.\")\n","\n","        except Exception as e:\n","            print(f\"Error during output preparation: {e}\")\n","            print(\"Proceeding to simplified fallback (no LLM corrections)...\")\n","            output_pdf = create_simplified_transcript(class_id, transcript_path)\n","            output_csv = create_simplified_csv(class_id, transcript_path)\n","            if output_pdf and output_csv:\n","                print(f\"\\nCreated simplified transcripts:\")\n","                print(f\"PDF: {output_pdf}\"); print(f\"CSV: {output_csv}\")\n","                print(\"\\n⚠️  Accuracy caution: Do not rely solely on this transcript. Manually verify key information.\")\n","            else:\n","                print(\"Failed to create simplified transcripts.\")\n","                return None, None\n","\n","        # Clean up temporary files\n","        try:\n","            temp_files = [fixed_path]  # converted WAV\n","\n","            # If we downloaded from a URL into /content, also clean that file\n","            try:\n","                if 'resolved_media_path' in locals() and resolved_media_path != audio_path:\n","                    if isinstance(resolved_media_path, str) and resolved_media_path.startswith(\"/content/\") and Path(resolved_media_path).exists():\n","                        temp_files.append(resolved_media_path)\n","            except Exception:\n","                pass\n","\n","            for temp_file in temp_files:\n","                try:\n","                    if temp_file and Path(temp_file).exists():\n","                        Path(temp_file).unlink()\n","                        print(f\"Cleaned up temporary file: {temp_file}\")\n","                except Exception as e:\n","                    print(f\"Note: Could not delete temp file {temp_file}: {e}\")\n","        except Exception as cleanup_error:\n","            print(f\"Note: Could not clean up temporary files: {str(cleanup_error)}\")\n","\n","        return output_pdf, output_csv\n","\n","    except Exception as e:\n","        print(f\"\\nERROR: {str(e)}\")\n","        if \"MP4\" in str(e) and audio_path.lower().endswith('.mp4'):\n","            print(\"\\nThere was a problem with your MP4 file. Suggestions:\")\n","            print(\"1. Convert it to MP3 on your computer before uploading\")\n","            print(\"2. Use a screen recorder to record Forum while playing back the class\")\n","            print(\"3. Contact Forum support about MP4 download issues\")\n","        else:\n","            print(\"\\nTranscription failed. Please try again with a different file.\")\n","        return None, None"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"p3BH0hikrkBQ","executionInfo":{"status":"aborted","timestamp":1755384311151,"user_tz":180,"elapsed":206291,"user":{"displayName":"Aleksei Korablev","userId":"02837062072924939756"}}},"outputs":[],"source":["# Run\n","outs = process_lecture(AUDIO_PATH, CLASS_ID, raw_curl, PRIVACY_MODE)\n","\n","# CUDA cleanup\n","try:\n","    torch.cuda.empty_cache()\n","except:\n","    pass\n","import gc; gc.collect()\n","print(\"CUDA cache cleared\")\n","\n","# Pretty print results\n","if outs:\n","    if len(outs) == 1:\n","        mode, pdfp, csvp = outs[0]\n","        print(f\"\\nSuccess! Your transcripts are ready ({mode}):\")\n","        print(f\"PDF: {pdfp}\")\n","        print(f\"CSV: {csvp}\")\n","    else:\n","        print(\"\\nSuccess! Your transcripts are ready (both privacy modes):\")\n","        for mode, pdfp, csvp in outs:\n","            print(f\"PDF ({mode}): {pdfp}\")\n","            print(f\"CSV ({mode}): {csvp}\")"]}],"metadata":{"accelerator":"GPU","colab":{"gpuType":"T4","provenance":[{"file_id":"1Z2nGiOo_yXhfLxy8DL93LmrfByhShiZY","timestamp":1755150496373},{"file_id":"1LcaKeGSpPnqXNMHZBX7FPzIo4bAmRuNP","timestamp":1754999391243},{"file_id":"1T3Jbu9KmoglVhKMorGYH1eikq1psXW0x","timestamp":1753457099254}]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"},"widgets":{"application/vnd.jupyter.widget-state+json":{"06b9bf52499b4651a8a8c1f9ae6fc84e":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_4c8319fde607431592c7634e847cd347","IPY_MODEL_4e66d5bbae8547d5a57afb4d473332f0","IPY_MODEL_10df34c71d864d5cbe92d53364aea36e"],"layout":"IPY_MODEL_1012e1bf20454845a3829c047e1f5fd7"}},"4c8319fde607431592c7634e847cd347":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_7e7c1ce8491d401496afec4013536540","placeholder":"​","style":"IPY_MODEL_12fd8c1fde6c44b59d64c5a676959244","value":"Processing segments:   0%"}},"4e66d5bbae8547d5a57afb4d473332f0":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"danger","description":"","description_tooltip":null,"layout":"IPY_MODEL_87c16581550c413c8bbce7b0479a60bc","max":1,"min":0,"orientation":"horizontal","style":"IPY_MODEL_0143542d5c3e4fd9a0c321e508e278b2","value":0}},"10df34c71d864d5cbe92d53364aea36e":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_cf3a18279e4c4f3aa6a192306f498506","placeholder":"​","style":"IPY_MODEL_f624eb9c3f2b44bda6b73d1791cbf0bf","value":" 0/1 [01:02&lt;?, ?segment/s]"}},"1012e1bf20454845a3829c047e1f5fd7":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"7e7c1ce8491d401496afec4013536540":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"12fd8c1fde6c44b59d64c5a676959244":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"87c16581550c413c8bbce7b0479a60bc":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"0143542d5c3e4fd9a0c321e508e278b2":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"cf3a18279e4c4f3aa6a192306f498506":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"f624eb9c3f2b44bda6b73d1791cbf0bf":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}}}}},"nbformat":4,"nbformat_minor":0}