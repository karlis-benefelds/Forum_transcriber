{"cells":[{"cell_type":"markdown","source":["<div style=\"border:2px solid #4F8EF7;padding:14px;border-radius:10px;background:#F5FAFF\">\n","<h2>Class Transcriber — Overview</h2>\n","<p>\n","This notebook transcribes class audio/video, pulls Forum metadata (title, section/schedule, date/time, attendance, events), and generates a polished PDF + CSV. It supports <b>Local</b>, <b>Google Drive</b>, and <b>Direct URL</b> inputs, and lets you save outputs to <b>Local</b> or <b>Google Drive</b>.\n","</p>\n","\n","<h3>Quick Start</h3>\n","<ol>\n","  <li>Runtime → <b>Change runtime type</b> → T4 GPU (or CPU if GPU not available).</li>\n","  <li>Runtime → <b>Run all</b>. The notebook will pause to ask for:\n","    <ul>\n","      <li><b>Forum cURL</b> (copied from DevTools → Network → “Copy as cURL”)</li>\n","      <li><b>Audio source</b>: local / gdrive / url</li>\n","      <li><b>Output destination</b>: local / gdrive</li>\n","      <li><b>Privacy</b>: names / ids / both</li>\n","    </ul>\n","  </li>\n","</ol>\n","\n","<h3>Important Notes</h3>\n","<ul>\n","  <li><b>Accuracy:</b> Machine transcripts can be imperfect—verify key details.</li>\n","  <li><b>URLs:</b> Signed links may expire. If a download fails, refresh the link.</li>\n","  <li><b>Privacy:</b> Use <code>ids</code> or <code>both</code> to anonymize student names.</li>\n","  <li><b>Drive:</b> Drive mounts only when you choose it for input or output.</li>\n","</ul>\n","</div>"],"metadata":{"id":"rYu0hqH45dTi"}},{"cell_type":"code","execution_count":null,"metadata":{"collapsed":true,"id":"eugbzc9F--tI","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1755476525260,"user_tz":180,"elapsed":12583,"user":{"displayName":"Aleksei Korablev","userId":"02837062072924939756"}},"outputId":"baeaf05f-bb06-4aec-9edd-2305e0d46806"},"outputs":[{"output_type":"stream","name":"stdout","text":["W: Skipping acquire of configured file 'main/source/Sources' as repository 'https://r2u.stat.illinois.edu/ubuntu jammy InRelease' does not seem to provide it (sources.list entry misspelt?)\n"]}],"source":["# =========================\n","#  IMPORTS & DEPENDENCIES\n","# =========================\n","\n","!pip install -q openai-whisper pydub requests iso8601 reportlab\n","!apt-get update -qq && apt-get install -y -qq ffmpeg"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"iZueWnhcXOom"},"outputs":[],"source":["import gc\n","import whisper\n","import json\n","import datetime\n","from pathlib import Path\n","import torch\n","from pydub import AudioSegment\n","import numpy as np\n","import requests\n","import re\n","import iso8601\n","import csv\n","import subprocess\n","from datetime import timedelta\n","from IPython.display import clear_output\n","from tqdm.notebook import tqdm  # For Jupyter/Colab"]},{"cell_type":"code","source":["# =========================\n","#  CONSTANTS & UTILITIES\n","#  (helpers, regex, spacing, labelers)\n","# =========================\n","\n","def extract_ids_from_curl(curl_text: str):\n","    \"\"\"\n","    Pull class/section/course IDs and the app link from the cURL.\n","    Prefers the Referer app URL; falls back to the API class URL.\n","    Returns dict: {\"course_id\": str|None, \"section_id\": str|None, \"class_id\": str|None, \"class_link\": str}\n","    \"\"\"\n","    # 1) Try Referer header (e.g., https://forum.minerva.edu/app/courses/2933/sections/11209/classes/79183)\n","    ref_match = re.search(r\"-H\\s+['\\\"](?:referer|Referer):\\s*([^'\\\"\\r\\n]+)\", curl_text)\n","    ref = ref_match.group(1).strip() if ref_match else \"\"\n","    class_link = \"\"\n","    course_id = section_id = class_id = None\n","\n","    if ref:\n","        m = re.search(r\"/app/courses/(\\d+)/sections/(\\d+)/classes/(\\d+)\", ref)\n","        if m:\n","            course_id, section_id, class_id = m.group(1), m.group(2), m.group(3)\n","            class_link = ref\n","\n","    # 2) Fallback: any API class URL in the cURL body\n","    if not class_id:\n","        m2 = re.search(r\"/api/v1/class_grader/classes/(\\d+)\", curl_text)\n","        if m2:\n","            class_id = m2.group(1)\n","            class_link = f\"https://forum.minerva.edu/app/classes/{class_id}\"\n","\n","    return {\n","        \"course_id\": course_id,\n","        \"section_id\": section_id,\n","        \"class_id\": class_id,\n","        \"class_link\": class_link\n","    }\n","\n","from contextlib import nullcontext  # at top if not present\n","\n","def download_to_temp(url: str) -> str:\n","    \"\"\"\n","    Download a remote media file (MP3/MP4/WAV/M4A/AAC/OGG) to /content and\n","    return the local path. Works with signed URLs that don't need cookies.\n","    \"\"\"\n","    base = url.split('?', 1)[0]\n","    suffix = Path(base).suffix or \".mp4\"\n","    local_name = f\"/content/input_from_url{suffix}\"\n","    print(f\"Downloading from URL: {url}\")\n","    with requests.get(url, stream=True) as r:\n","        r.raise_for_status()\n","        with open(local_name, \"wb\") as f:\n","            for chunk in r.iter_content(chunk_size=1024 * 1024):\n","                if chunk:\n","                    f.write(chunk)\n","    print(f\"Downloaded URL to: {local_name}\")\n","    return local_name\n","\n","def resolve_audio_input() -> str:\n","    \"\"\"\n","    Ask the user how to provide the audio/video: local upload, Google Drive, or URL.\n","    - local: user uploads in the Colab sidebar then provides a /content/... path\n","    - gdrive: lazily import & mount google.colab.drive, then prompt for Drive path\n","    - url: download file to /content and return the local temp path\n","    \"\"\"\n","    print(\"2) Choose audio source: local  |  gdrive  |  url\")\n","    src = input(\"Source [local/gdrive/url]: \").strip().lower()\n","    clear_output()\n","\n","    if src == \"gdrive\":\n","        print(\"Mounting Google Drive (one-time authorization may be required)...\")\n","        # Lazy import ONLY if user picked Google Drive\n","        from google.colab import drive   # imported here to avoid forcing the dependency otherwise\n","        drive.mount(\"/content/drive\")\n","        print(\"Enter the file path inside Drive (e.g., /content/drive/MyDrive/recordings/lecture.mp4)\")\n","        path = input(\"Drive path: \").strip()\n","        clear_output()\n","        return path\n","\n","    if src == \"url\":\n","        print(\"Paste the direct file URL (signed URLs supported):\")\n","        url = input(\"URL: \").strip()\n","        clear_output()\n","        return download_to_temp(url)\n","\n","    # default → local upload path\n","    print(\"After uploading your file to Colab, enter its path\")\n","    print(\"   (Typically this will be '/content/your_file.mp3' or '/content/your_file.mp4')\")\n","    print()\n","    print(\"‼️  Wait for the upload to finish before submitting its path.\")\n","    print(\"   Track progress in the bottom-left corner.\")\n","    print()\n","    path = input(\"Path: \").strip()\n","    clear_output()\n","    return path\n","\n","def free_cuda_mem():\n","    \"\"\"Safely clear CUDA cache and trigger Python GC.\"\"\"\n","    try:\n","        if torch.cuda.is_available():\n","            torch.cuda.empty_cache()\n","    except Exception:\n","        pass\n","    try:\n","        gc.collect()\n","    except Exception:\n","        pass\n","\n","def resolve_output_destination() -> str:\n","    \"\"\"\n","    Ask where to save outputs (PDF/CSV): local folder or Google Drive folder.\n","    Lazily mounts Drive only if chosen. Ensures the folder exists.\n","    Returns an absolute folder path.\n","    \"\"\"\n","    print(\"Where should I save the outputs? local  |  gdrive\")\n","    dest = input(\"Destination [local/gdrive]: \").strip().lower()\n","    clear_output()\n","\n","    if dest == \"gdrive\":\n","        # Lazy import/mount only when needed\n","        from google.colab import drive\n","        print(\"Mounting Google Drive...\")\n","        drive.mount(\"/content/drive\")\n","        print(\"Enter a Drive folder (e.g., /content/drive/MyDrive/Transcripts)\")\n","        out_dir = input(\"Drive folder: \").strip()\n","        clear_output()\n","        if not out_dir:\n","            out_dir = \"/content/drive/MyDrive\"\n","        Path(out_dir).mkdir(parents=True, exist_ok=True)\n","        return out_dir\n","\n","    # default: local\n","    print(\"Enter a local folder (press Enter for /content)\")\n","    out_dir = input(\"Local folder: \").strip() or \"/content\"\n","    clear_output()\n","    Path(out_dir).mkdir(parents=True, exist_ok=True)\n","    return out_dir"],"metadata":{"id":"ZBI8zO3oC7nU"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["<div style=\"border:2px solid #444;padding:12px;border-radius:8px;background:#FBFBFB\">\n","<h3>User Prompts</h3>\n","<p>\n","You’ll paste your Forum cURL, choose where your audio/video comes from (local / Google Drive / direct URL), decide where to save outputs (local / Google Drive), and set privacy (names / ids / both).\n","</p>\n","<ul>\n","  <li><b>Class ID auto-detected</b> from your cURL — no need to type it.</li>\n","  <li><b>Direct URL</b> option will download the media automatically.</li>\n","  <li><b>Outputs</b> (PDF/CSV) go to the folder you select, regardless of input source.</li>\n","</ul>\n","</div>"],"metadata":{"id":"_1Jt7tWmA69E"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"Zz_o4ed-Unr8","colab":{"base_uri":"https://localhost:8080/"},"outputId":"ba23a0d6-9507-4bc3-eb4e-a2d9ee34cebc","executionInfo":{"status":"ok","timestamp":1755476625683,"user_tz":180,"elapsed":100408,"user":{"displayName":"Aleksei Korablev","userId":"02837062072924939756"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["Thank you! Here's what you provided:\n","\n","Class ID: 81239\n","Audio Path: /content/input_from_url.mp4\n","Outputs Folder: /content/drive/MyDrive/AI Curriculum Projects Summer 2025/Transcriptions/Alex/v2.3\n","Privacy Mode: both\n","\n","Starting the transcript generation process...\n","⏳ Expect this to take about 15 minutes (varies with file length)\n"]}],"source":["print(\"1) Paste your Forum cURL (right-click in Chrome DevTools → Copy as cURL):\")\n","raw_curl = input().strip()\n","clear_output()\n","\n","# Auto-derive Class ID (fallback to API URL if Referer is missing)\n","_ids = extract_ids_from_curl(raw_curl)\n","CLASS_ID = _ids.get(\"class_id\") or (re.search(r\"/api/v1/class_grader/classes/(\\d+)\", raw_curl).group(1)\n","                                    if re.search(r\"/api/v1/class_grader/classes/(\\d+)\", raw_curl) else None)\n","if not CLASS_ID:\n","    raise ValueError(\n","        \"Could not extract Class ID from your cURL. \"\n","        \"Open the class page and copy a request that includes a Referer like \"\n","        \"https://forum.minerva.edu/app/courses/.../sections/.../classes/<ID>, \"\n","        \"or a class API URL.\"\n","    )\n","\n","# NEW: unified input selection (local / Google Drive / direct URL)\n","AUDIO_PATH = resolve_audio_input()\n","\n","# NEW: choose where outputs (PDF/CSV) will be saved\n","OUTPUT_DIR = resolve_output_destination()\n","\n","print(\"3) Student name privacy\")\n","print(\"   Type one of: names  (show names)  |  ids  (anonymize to IDs)  |  both  (generate both)\")\n","PRIVACY_MODE = input(\"Privacy mode [names/ids/both]: \").strip().lower()\n","if PRIVACY_MODE not in (\"names\", \"ids\", \"both\"):\n","    PRIVACY_MODE = \"names\"\n","clear_output()\n","\n","print(\"Thank you! Here's what you provided:\\n\")\n","print(f\"Class ID: {CLASS_ID}\")\n","print(f\"Audio Path: {AUDIO_PATH}\")\n","print(f\"Outputs Folder: {OUTPUT_DIR}\")\n","print(f\"Privacy Mode: {PRIVACY_MODE}\")\n","print()\n","print(\"Starting the transcript generation process...\")\n","print(\"⏳ Expect this to take about 15 minutes (varies with file length)\")"]},{"cell_type":"markdown","source":["<div style=\"border:2px solid #2E86C1;padding:12px;border-radius:8px;background:#F3F9FE\">\n","<h3>Audio Prep & Transcription</h3>\n","<p>\n","Validates/normalizes audio (MP3/MP4/WAV/M4A/AAC/OGG), converts to a Whisper-optimized WAV, and transcribes in chunks to handle long recordings efficiently.\n","</p>\n","<ul>\n","  <li>Uses <b>torch.amp.autocast('cuda', ...)</b> (no deprecation warnings).</li>\n","  <li>Fixes common text issues (glued sentences, spacing).</li>\n","  <li>GPU (T4) recommended but not required.</li>\n","</ul>\n","</div>"],"metadata":{"id":"_9fUSKqLBV1S"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"DASfAmyYPzWu"},"outputs":[],"source":["class AudioPreprocessor:\n","    @staticmethod\n","    def validate_and_fix_file(file_path: str) -> str:\n","        \"\"\"\n","        Validates and preprocesses audio files for optimal transcription.\n","        For MP4 files, first converts to MP3 as an intermediate step.\n","        \"\"\"\n","        print(f\"Validating file: {file_path}\")\n","        if not Path(file_path).exists():\n","            raise FileNotFoundError(f\"File not found: {file_path}\")\n","\n","        try:\n","            # If MP4, convert to MP3 (more tolerant), then to 16kHz mono WAV\n","            if file_path.lower().endswith('.mp4'):\n","                print(f\"Converting MP4 to MP3 (intermediate step)...\")\n","                mp3_path = file_path.rsplit('.', 1)[0] + '.mp3'\n","                result = subprocess.run([\n","                    'ffmpeg', '-y', '-v', 'warning', '-xerror',\n","                    '-i', file_path, '-vn',\n","                    '-acodec', 'libmp3lame', '-ar', '44100', '-ab', '192k', '-f', 'mp3',\n","                    mp3_path\n","                ], capture_output=True, text=True, check=False)\n","\n","                if result.returncode == 0 and Path(mp3_path).exists() and Path(mp3_path).stat().st_size > 0:\n","                    print(f\"Successfully converted to MP3: {mp3_path}\")\n","                    return AudioPreprocessor._convert_to_whisper_wav(mp3_path)\n","                else:\n","                    print(f\"MP3 conversion failed with error: {result.stderr}\")\n","                    return AudioPreprocessor._python_extract_audio(file_path)\n","\n","            # Common audio formats → Whisper WAV\n","            elif file_path.lower().endswith(('.mp3', '.m4a', '.aac', '.ogg')):\n","                print(f\"Converting audio file to optimized WAV format...\")\n","                return AudioPreprocessor._convert_to_whisper_wav(file_path)\n","\n","            elif file_path.lower().endswith('.wav'):\n","                print(f\"File is already in WAV format: {file_path}\")\n","                return file_path\n","\n","            else:\n","                raise ValueError(f\"Unsupported file format: {file_path}. Please upload MP3, MP4, WAV, or other common audio/video format.\")\n","\n","        except Exception as e:\n","            print(f\"Error during file processing: {str(e)}\")\n","            raise\n","\n","    @staticmethod\n","    def _convert_to_whisper_wav(audio_path: str) -> str:\n","        \"\"\"Convert any audio file to WAV format optimized for Whisper (16kHz mono, s16le).\"\"\"\n","        wav_path = audio_path.rsplit('.', 1)[0] + '.wav'\n","        try:\n","            subprocess.run([\n","                'ffmpeg', '-y', '-i', audio_path,\n","                '-acodec', 'pcm_s16le', '-ar', '16000', '-ac', '1',\n","                wav_path\n","            ], capture_output=True, text=True, check=True)\n","            print(f\"Successfully converted to Whisper-optimized WAV: {wav_path}\")\n","            return wav_path\n","        except subprocess.CalledProcessError as e:\n","            print(f\"WAV conversion failed. ffmpeg error: {e.stderr}\")\n","            raise RuntimeError(f\"Failed to convert {audio_path} to WAV format\")\n","\n","    @staticmethod\n","    def _python_extract_audio(file_path: str) -> str:\n","        \"\"\"Fallback extraction using PyDub when ffmpeg CLI fails.\"\"\"\n","        print(\"Attempting Python-based audio extraction...\")\n","        wav_path = file_path.rsplit('.', 1)[0] + '_extracted.wav'\n","        try:\n","            audio = AudioSegment.from_file(file_path).set_frame_rate(16000).set_channels(1).set_sample_width(2)\n","            audio.export(wav_path, format=\"wav\")\n","            if Path(wav_path).exists() and Path(wav_path).stat().st_size > 0:\n","                print(f\"Successfully extracted audio using Python: {wav_path}\")\n","                return wav_path\n","        except Exception as e:\n","            print(f\"Python audio extraction failed: {str(e)}\")\n","\n","        print(\"Attempting direct binary extraction...\")\n","        try:\n","            binary_wav_path = file_path.rsplit('.', 1)[0] + '_binary.wav'\n","            result = subprocess.run([\n","                'ffmpeg', '-y',\n","                '-f', 'lavfi', '-i', f\"movie={file_path}[out+audio]\",\n","                '-vn', '-acodec', 'pcm_s16le', '-ar', '16000', '-ac', '1',\n","                binary_wav_path\n","            ], capture_output=True, text=True, check=False)\n","\n","            if result.returncode == 0 and Path(binary_wav_path).exists() and Path(binary_wav_path).stat().st_size > 0:\n","                print(f\"Binary extraction successful: {binary_wav_path}\")\n","                return binary_wav_path\n","        except Exception as e2:\n","            print(f\"Binary extraction failed: {str(e2)}\")\n","\n","        raise RuntimeError(\"All audio extraction methods failed. Please convert the file manually before uploading.\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"yAdjGA9VVV_2"},"outputs":[],"source":["class TranscriptionProcessor:\n","    \"\"\"\n","    Chunked Whisper transcription with GPU-friendly settings.\n","    Splits long audio into ~4-hour segments, transcribes, and stitches results.\n","\n","    Args:\n","        segment_length (int): Segment length in seconds (default ~4 hours).\n","        model_name (str): Whisper model name to load.\n","    \"\"\"\n","    def __init__(self, segment_length: int = 14_400, model_name: str = \"medium\"):\n","        self.device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n","        print(f\"Using device: {self.device}\")\n","\n","        if self.device == \"cuda\":\n","            # CUDA perf tweaks\n","            torch.backends.cuda.matmul.allow_tf32 = True\n","            torch.backends.cudnn.benchmark = True\n","            torch.backends.cudnn.allow_tf32 = True\n","            torch.backends.cudnn.deterministic = False\n","            torch.cuda.empty_cache()\n","            # Be careful with this fraction in multi-process environments\n","            try:\n","                torch.cuda.set_per_process_memory_fraction(0.9)\n","            except Exception as e:\n","                print(f\"Warning: could not set memory fraction: {e}\")\n","\n","        print(\"Loading Whisper model...\")\n","        self.model = whisper.load_model(model_name).to(self.device)\n","        if self.device == \"cuda\":\n","            # Use FP16 on GPU\n","            self.model = self.model.half()\n","\n","        self.segment_length = int(segment_length)\n","\n","    def transcribe(self, audio_path: str, class_id: str) -> str | None:\n","        \"\"\"\n","        Transcribe `audio_path` into /content/session_{class_id}_transcript.json and return that path.\n","\n","        Args:\n","            audio_path (str): Path to an audio/video file supported by pydub/ffmpeg.\n","            class_id (str): Identifier used to name the output transcript.\n","\n","        Returns:\n","            str | None: Path to the resulting JSON transcript, or None if nothing was transcribed.\n","        \"\"\"\n","        print(\"Processing audio to generate transcript JSON...\")\n","        try:\n","            # Load audio\n","            audio = AudioSegment.from_file(audio_path)\n","            total_duration = len(audio) / 1000.0  # seconds (float)\n","            print(f\"Total duration: {timedelta(seconds=int(total_duration))}\")\n","\n","            all_segments: list[dict] = []\n","            segment_times = range(0, int(total_duration), self.segment_length)\n","\n","            for start_time in tqdm(segment_times, desc=\"Processing segments\", unit=\"segment\"):\n","                # Compute this segment's end (in seconds)\n","                remaining = total_duration - start_time\n","                duration = min(self.segment_length, remaining)\n","\n","                # Slice pydub audio in milliseconds (cast to int to be safe)\n","                start_ms = int(start_time * 1000)\n","                end_ms = int((start_time + duration) * 1000)\n","                segment = audio[start_ms:end_ms]\n","\n","                # Export temp wav for Whisper\n","                temp_path = f\"/content/temp_segment_{start_time}.wav\"\n","                segment.export(temp_path, format=\"wav\")\n","\n","                try:\n","                    # Use autocast only on CUDA; nullcontext() on CPU\n","                    cast_ctx = torch.amp.autocast(\"cuda\") if self.device == \"cuda\" else nullcontext()\n","                    with cast_ctx:\n","                        result = self.model.transcribe(\n","                            temp_path,\n","                            word_timestamps=True,\n","                            language=\"en\",\n","                            task=\"transcribe\",\n","                            fp16=(self.device == \"cuda\"),\n","                            condition_on_previous_text=True,\n","                            initial_prompt=\"This is a university lecture.\"\n","                        )\n","\n","                        # Gather segments with global (shifted) times\n","                        for seg in result.get(\"segments\", []):\n","                            seg_start = float(seg.get(\"start\", 0.0)) + start_time\n","                            seg_end = float(seg.get(\"end\", 0.0)) + start_time\n","\n","                            words = []\n","                            for w in seg.get(\"words\", []) or []:\n","                                words.append({\n","                                    \"word\": str(w.get(\"word\", \"\")).strip(),\n","                                    \"start\": float(w.get(\"start\", 0.0)) + start_time,\n","                                    \"end\": float(w.get(\"end\", 0.0)) + start_time\n","                                })\n","\n","                            all_segments.append({\n","                                \"start\": seg_start,\n","                                \"end\": seg_end,\n","                                \"text\": normalize_sentence_spacing(str(seg.get(\"text\", \"\")).strip()),\n","                                \"words\": words\n","                            })\n","\n","                except Exception as segment_error:\n","                    print(f\"Error processing segment at {start_time}s: {segment_error}\")\n","                    continue\n","                finally:\n","                    # Cleanup temp file and free GPU mem\n","                    try:\n","                        Path(temp_path).unlink(missing_ok=True)\n","                    except Exception as e:\n","                        print(f\"Warning: Failed to delete temp file {temp_path}: {e}\")\n","                    if self.device == \"cuda\":\n","                        torch.cuda.empty_cache()\n","                        gc.collect()\n","\n","            if not all_segments:\n","                print(\"Warning: No segments were successfully transcribed!\")\n","                return None\n","\n","            # Sort by start time and write JSON\n","            transcript_path = f\"/content/session_{class_id}_transcript.json\"\n","            with open(transcript_path, \"w\", encoding=\"utf-8\") as f:\n","                json.dump({\"segments\": sorted(all_segments, key=lambda x: x[\"start\"])}, f, indent=2)\n","\n","            print(f\"\\nTranscript JSON saved to: {transcript_path}\")\n","            return transcript_path\n","\n","        except Exception as e:\n","            print(f\"Error in transcription process: {e}\")\n","            raise"]},{"cell_type":"markdown","source":["<div style=\"border:2px solid #8E44AD;padding:12px;border-radius:8px;background:#F9F3FF\">\n","<h3>Forum API Fetch</h3>\n","<p>\n","Fetches class metadata (session title, section/schedule), recording time, attendance, voice windows, and timeline events. These drive the header, attendance table, event table, and the event-based transcript grouping in the PDF/CSV.\n","</p>\n","<ul>\n","  <li>If some fields are missing, sensible fallbacks are used (e.g., speaker shown as “Professor”).</li>\n","  <li>Attendance table highlights <b>Present</b> (green) and <b>Absent</b> (red).</li>\n","</ul>\n","</div>"],"metadata":{"id":"4WuOo8iSBJCY"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"nFRSWCdOSfo_"},"outputs":[],"source":["def clean_curl(curl_string):\n","    \"\"\"Parse a DevTools cURL and return an HTTP headers dict including Cookie (if present).\"\"\"\n","    headers = {}\n","    header_matches = re.findall(r\"-H ['\\\"](.*?): (.*?)['\\\"]\", curl_string)\n","    for name, value in header_matches:\n","        headers[name] = value\n","    cookie_match = re.search(r\"-b ['\\\"](.*?)['\\\"]\", curl_string)\n","    if cookie_match:\n","        headers['Cookie'] = cookie_match.group(1)\n","    return headers\n","\n","def get_forum_events(class_id, headers):\n","    \"\"\"\n","    Fetch class meta + class events from Forum and normalize.\n","\n","    Also extracts a student attendance list (present/absent).\n","    \"\"\"\n","    print(\"Fetching class and event data from Forum...\")\n","\n","    # ---- Class meta ----\n","    class_url = f'https://forum.minerva.edu/api/v1/class_grader/classes/{class_id}'\n","    print(f\"Requesting class data from: {class_url}\")\n","    r = requests.get(class_url, headers=headers)\n","    if r.status_code != 200:\n","        print(f\"Error accessing class data. Status code: {r.status_code}\\n{r.text}\")\n","        raise RuntimeError(f\"Failed to access class data. Status code: {r.status_code}\")\n","    data = r.json()\n","\n","    session_title = data.get('title') or f\"Session {class_id}\"\n","    course_obj = (data.get('section') or {}).get('course') or {}\n","    course_code  = course_obj.get('course-code', '')\n","    course_title = course_obj.get('title', '')\n","    section_title = (data.get('section') or {}).get('title', '')\n","    class_type = data.get('type', '')\n","    rec = (data.get('recording-sessions') or [{}])[0]\n","    recording_start = rec.get('recording-started')\n","    recording_end   = rec.get('recording-ended')\n","\n","    schedule_guess = ''\n","    if isinstance(section_title, str) and ',' in section_title:\n","        parts = [p.strip() for p in section_title.split(',', 1)]\n","        schedule_guess = parts[1] if len(parts) > 1 else ''\n","\n","    class_meta = {\n","        'session_title': session_title,\n","        'course_code': course_code,\n","        'course_title': course_title,\n","        'section_title': section_title,\n","        'schedule': schedule_guess,\n","        'class_type': class_type,\n","        'recording_start': recording_start,\n","        'recording_end': recording_end,\n","    }\n","\n","    if not recording_start:\n","        raise KeyError(\"No recording-started found in class data\")\n","\n","    # ---- Events ----\n","    events_url = f'https://forum.minerva.edu/api/v1/class_grader/classes/{class_id}/class-events'\n","    print(f\"Requesting events from: {events_url}\")\n","    r = requests.get(events_url, headers=headers)\n","    if r.status_code != 200:\n","        print(f\"Error accessing class events. Status code: {r.status_code}\\n{r.text}\")\n","        raise RuntimeError(f\"Failed to access class events. Status code: {r.status_code}\")\n","    events = r.json()\n","    if not isinstance(events, list):\n","        raise ValueError(\"No valid class events returned from API\")\n","\n","    voice_events = []\n","    timeline_segments = []\n","    ref_time = iso8601.parse_date(recording_start)\n","\n","    for ev in events:\n","        et = ev.get('event-type')\n","        try:\n","            if et == 'voice':\n","                duration_ms = (ev.get('event-data') or {}).get('duration', 0)\n","                duration = duration_ms / 1000.0\n","                if duration >= 1:\n","                    start_time = iso8601.parse_date(ev['start-time'])\n","                    end_time   = iso8601.parse_date(ev['end-time'])\n","                    voice_events.append({\n","                        'start': (start_time - ref_time).total_seconds(),\n","                        'end': (end_time - ref_time).total_seconds(),\n","                        'duration': duration,\n","                        'speaker': {\n","                          'id':        (ev.get('actor') or {}).get('id') or (ev.get('actor') or {}).get('user-id') or ((ev.get('actor') or {}).get('user') or {}).get('id'),\n","                          'first-name': (ev.get('actor') or {}).get('first-name'),\n","                          'last-name':  (ev.get('actor') or {}).get('last-name')\n","                        }\n","                    })\n","            elif et == 'timeline-segment':\n","                start_time = iso8601.parse_date(ev['start-time'])\n","                seg = (ev.get('event-data') or {})\n","                timeline_segments.append({\n","                    'abs_start': ev['start-time'],\n","                    'offset_seconds': (start_time - ref_time).total_seconds(),\n","                    'section': seg.get('timeline-section-title', ''),\n","                    'title':   seg.get('timeline-segment-title', ''),\n","                })\n","        except KeyError:\n","            continue\n","\n","    timeline_segments.sort(key=lambda x: x['offset_seconds'])\n","\n","    # ---- Attendance ----\n","    attendance = []\n","    for cu in (data.get('class-users') or []):\n","        role = (cu.get('role') or '').lower()\n","        if role == 'student':\n","            u = cu.get('user') or {}\n","            first = u.get('first-name', '') or ''\n","            last = u.get('last-name', '') or ''\n","            name = f\"{first} {last}\".strip() or (u.get('preferred-name') or '').strip() or (u.get('first-name') or '').strip()\n","            uid = u.get('id') or u.get('user-id')\n","            absent = bool(cu.get('absent', False))\n","            attendance.append({'id': uid, 'name': name, 'absent': absent})\n","\n","    try:\n","        attendance.sort(key=lambda x: (x['name'] or '').lower())\n","    except Exception:\n","        pass\n","\n","    events_data = {\n","        'class_id': class_id,\n","        'class_meta': class_meta,\n","        'voice_events': voice_events,\n","        'timeline_segments': timeline_segments,\n","        'attendance': attendance\n","    }\n","    temp_events_path = f\"/content/session_{class_id}_events.json\"\n","    with open(temp_events_path, 'w', encoding='utf-8') as f:\n","        json.dump(events_data, f, indent=2)\n","\n","    print(f\"Processed voice events: {len(voice_events)}; timeline segments: {len(timeline_segments)}\")\n","    return events_data\n","\n","# ----------------------------\n","# Utilities\n","# ----------------------------\n","def _fmt_mmss(seconds_float):\n","    if seconds_float is None:\n","        return \"\"\n","    seconds = max(0, int(seconds_float))\n","    m, s = divmod(seconds, 60)\n","    return f\"{m:02d}:{s:02d}\"\n","\n","def _safe_date(date_str):\n","    if not date_str:\n","        return \"\"\n","    try:\n","        return date_str.split('T')[0]\n","    except Exception:\n","        return \"\"\n","\n","def _fmt_dt_hm(dt_str: str) -> str:\n","    \"\"\"YYYY-MM-DD HH:MM TZ from ISO8601; fallback to YYYY-MM-DD.\"\"\"\n","    if not dt_str:\n","        return \"\"\n","    try:\n","        dt = iso8601.parse_date(dt_str)\n","        return dt.strftime(\"%Y-%m-%d %H:%M %Z\")\n","    except Exception:\n","        return _safe_date(dt_str)\n","\n","def soft_break_long_token(s: str, max_run: int = 14) -> str:\n","    if not s:\n","        return s\n","    # use a callable to avoid backslash escapes in the replacement\n","    pat = re.compile(r'(\\S{%d})(?=\\S)' % max_run)\n","    return pat.sub(lambda m: m.group(1) + '\\u200b', s)\n","\n","# --- Text cleanup helper ---\n","\n","def normalize_sentence_spacing(text: str) -> str:\n","    \"\"\"Fix glued sentences & punctuation spacing (respects ellipses), collapse newlines/spaces.\"\"\"\n","    if not text:\n","        return text\n","    # remove zero-widths & non-breaking spaces, collapse newlines\n","    text = re.sub(r'[\\u200B-\\u200D\\uFEFF]', '', text)\n","    text = text.replace('\\u00A0', ' ')\n","    text = re.sub(r'\\s*\\n+\\s*', ' ', text)\n","\n","    # keep ellipses intact but add a space after if glued\n","    text = re.sub(r'(\\.\\.\\.)(?=\\S)', r'\\1 ', text)\n","\n","    # add a space after ., ?, ! when next visible char is a letter or an opening quote/paren\n","    # and NOT a decimal number (next char digit)\n","    text = re.sub(r'(?<!\\.)'              # previous char is not a dot (avoid inside \"...\")\n","                  r'([.!?])'              # sentence end\n","                  r'(?=([\"“\\'(\\[]?[A-Za-z]))'  # next visible char is letter (maybe after quote/paren)\n","                  , r'\\1 ', text)\n","\n","    # optional: add space after : or ; if followed by a letter\n","    text = re.sub(r'([:;])(?=([\"“\\'(\\[]?[A-Za-z]))', r'\\1 ', text)\n","\n","    # de-glue common quote cases like: .“Word  .’Word  .)Word\n","    text = re.sub(r'([.!?][\"”\\')\\]])(?=\\S)', r'\\1 ', text)\n","\n","    # collapse multiple spaces\n","    text = re.sub(r'\\s{2,}', ' ', text)\n","    return text.strip()\n","\n","def label_from_actor(actor: dict, name_mode: str, student_ids: set | None = None) -> str:\n","    \"\"\"Return display label for a speaker based on privacy setting.\"\"\"\n","    if not isinstance(actor, dict):\n","        return \"Professor\"\n","    uid = actor.get('id') or actor.get('user-id') or (actor.get('user') or {}).get('id')\n","    fn  = (actor.get('first-name') or '').strip()\n","    ln  = (actor.get('last-name') or '').strip()\n","    full = f\"{fn} {ln}\".strip()\n","    if name_mode == 'ids':\n","        # anonymize students; if we don't know role, anonymize when in the student set (if provided)\n","        if (student_ids is None and uid is not None) or (student_ids is not None and uid in student_ids):\n","            return str(uid) if uid is not None else \"ID\"\n","    return full or \"Professor\""]},{"cell_type":"markdown","source":["<div style=\"border:2px solid #27AE60;padding:12px;border-radius:8px;background:#F4FBF6\">\n","<h3>Output Compilers (PDF/CSV) & Fallbacks</h3>\n","<p>\n","Builds the final PDF and CSV with a centered header (title + section/schedule), then left-aligned <b>Class ID</b>, <b>Class Date/Time</b>, <b>Class Link</b>, followed by Attendance, Class Events, and an event-grouped transcript (timestamps, speakers, text).\n","</p>\n","<ul>\n","  <li><b>Privacy modes:</b> names / ids / both (generates two sets: <code>_names</code> and <code>_ids</code>).</li>\n","  <li>Outputs are saved to your chosen folder (local or Google Drive).</li>\n","  <li>Fallbacks produce simplified PDF/CSV if Forum data is unavailable.</li>\n","</ul>\n","</div>"],"metadata":{"id":"YsmeobBDBuBl"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"YtDyII9cVXcw"},"outputs":[],"source":["from reportlab.lib import colors\n","from reportlab.lib.pagesizes import letter\n","from reportlab.platypus import SimpleDocTemplate, Table, TableStyle, Paragraph, Spacer\n","from reportlab.lib.styles import getSampleStyleSheet, ParagraphStyle\n","from reportlab.lib.units import inch\n","\n","def compile_transcript_to_pdf(class_id, headers, name_mode: str = \"names\", file_suffix: str = \"\", output_dir: str = \"/content\"):\n","    \"\"\"\n","    Render the PDF with header, attendance, events, and event-bucketed transcript.\n","    \"\"\"\n","    try:\n","        with open(f\"/content/session_{class_id}_transcript.json\", 'r') as f:\n","            transcript_data = json.load(f)\n","        with open(f\"/content/session_{class_id}_events.json\", 'r') as f:\n","            events_data = json.load(f)\n","\n","        class_meta = events_data.get('class_meta', {})\n","        timeline_segments = events_data.get('timeline_segments', [])\n","        attendance = events_data.get('attendance', [])\n","        student_ids_set = {a.get('id') for a in attendance if a.get('id') is not None}\n","\n","        # Build speaker map from voice windows\n","        speaker_map = {}\n","        for ev in events_data.get('voice_events', []):\n","          speaker_map[(ev['start'], ev['end'])] = ev.get('speaker', {})  # keep dict (has id + names)\n","\n","        def find_speaker_at_time(t):\n","          for (start, end), spk in speaker_map.items():\n","              if start <= t <= end:\n","                  return label_from_actor(spk, name_mode, student_ids_set)\n","          return \"Professor\"\n","\n","\n","        # Combine consecutive segments by same speaker\n","        compiled_entries = []\n","        current = {'speaker': None, 'start_time': None, 'text': [], 'end_time': None}\n","\n","        for seg in transcript_data['segments']:\n","            st, en, tx = seg['start'], seg['end'], seg['text'].strip()\n","            if not tx:\n","                continue\n","            spk = find_speaker_at_time(st)\n","\n","            start_new = False\n","            if not current['speaker']:\n","                start_new = True\n","            elif current['speaker'] != spk:\n","                start_new = True\n","            elif current['end_time'] is not None and st - current['end_time'] > 2:\n","                start_new = True\n","\n","            if start_new:\n","                if current['speaker']:\n","                    compiled_entries.append(current)\n","                current = {'speaker': spk, 'start_time': st, 'text': [tx], 'end_time': en}\n","            else:\n","                current['text'].append(tx)\n","                current['end_time'] = en\n","\n","        if current['speaker']:\n","            compiled_entries.append(current)\n","\n","        # Styles\n","        styles = getSampleStyleSheet()\n","        contribution_style = ParagraphStyle(\n","            'ContributionStyle', parent=styles['Normal'],\n","            fontName='Helvetica', fontSize=10, leading=12, wordWrap='CJK'\n","        )\n","        header_style = ParagraphStyle(\n","            'HeaderStyle', parent=styles['Normal'],\n","            fontName='Helvetica-Bold', fontSize=12, textColor=colors.whitesmoke, alignment=1\n","        )\n","        speaker_style = ParagraphStyle(\n","            'SpeakerStyle', parent=styles['Normal'],\n","            fontName='Helvetica', fontSize=10, leading=12, wordWrap='CJK'\n","        )\n","\n","        # PDF scaffold\n","        output_path = str(Path(output_dir) / f\"session_{class_id}_transcript{file_suffix}.pdf\")\n","        doc = SimpleDocTemplate(output_path, pagesize=letter,\n","                                rightMargin=72, leftMargin=72, topMargin=72, bottomMargin=72)\n","        elements = []\n","\n","        # ====== HEADER ======\n","        session_line = class_meta.get('session_title') or f\"Session {class_id}\"\n","        elements.append(Paragraph(session_line, styles['Title']))\n","\n","        # Only the sec_sched value, centered\n","        sec_sched = class_meta.get('section_title', '') or class_meta.get('schedule', '')\n","        if sec_sched:\n","            centered_info_style = ParagraphStyle('CenteredInfo', parent=styles['Heading3'], alignment=1)\n","            elements.append(Paragraph(sec_sched, centered_info_style))\n","            elements.append(Spacer(1, 12))\n","        else:\n","            elements.append(Spacer(1, 12))\n","\n","        # Left-aligned: labeled class id, date/time, and class link from cURL Referer\n","        left_info_style = ParagraphStyle('LeftInfo', parent=styles['Normal'], alignment=0)\n","        class_datetime = _fmt_dt_hm(class_meta.get('recording_start'))\n","\n","        # Prefer the Referer header for the app URL (e.g., https://forum.minerva.edu/app/courses/.../classes/...)\n","        ref = (headers.get('referer') or headers.get('Referer') or '').strip()\n","        m = re.search(r'https://forum\\.minerva\\.edu/app/[^\\s\"\\']+', ref)\n","        class_link = m.group(0) if m else f\"https://forum.minerva.edu/app/classes/{class_id}\"\n","\n","        elements.append(Paragraph(f\"<b>Class ID:</b> {class_id}\", left_info_style))\n","        elements.append(Paragraph(f\"<b>Class Date/Time:</b> {class_datetime}\", left_info_style))\n","        elements.append(Paragraph(f'<b>Class Link:</b> <a href=\"{class_link}\">{class_link}</a>', left_info_style))\n","        elements.append(Spacer(1, 12))  # blank line before next section\n","\n","        # ====== ATTENDANCE TABLE ======\n","        attendance = events_data.get('attendance', [])\n","        if attendance:\n","            elements.append(Paragraph(\"Attendance\", styles['Heading3']))\n","            att_rows = [[Paragraph('Student', header_style), Paragraph('Status', header_style)]]\n","            for a in attendance:\n","              status = 'Absent' if a.get('absent') else 'Present'\n","              display_student = (f\"ID {a.get('id')}\" if name_mode == 'ids' and a.get('id') is not None else a.get('name',''))\n","              att_rows.append([Paragraph(soft_break_long_token(display_student, 14), speaker_style), status])\n","            att_table = Table(att_rows, colWidths=[4.5*inch, 1.5*inch], repeatRows=1)\n","            att_style = TableStyle([\n","                ('BACKGROUND', (0,0), (-1,0), colors.grey),\n","                ('TEXTCOLOR', (0,0), (-1,0), colors.whitesmoke),\n","                ('ALIGN', (0,0), (-1,0), 'CENTER'),\n","                ('FONTNAME', (0,0), (-1,0), 'Helvetica-Bold'),\n","                ('FONTNAME', (0,1), (-1,-1), 'Helvetica'),\n","                ('FONTSIZE', (0,1), (-1,-1), 10),\n","                ('VALIGN', (0,0), (-1,-1), 'TOP'),\n","                ('GRID', (0,0), (-1,-1), 1, colors.black),\n","                ('LEFTPADDING', (0,0), (-1,-1), 6),\n","                ('RIGHTPADDING', (0,0), (-1,-1), 6),\n","                ('TOPPADDING', (0,0), (-1,-1), 3),\n","                ('BOTTOMPADDING', (0,0), (-1,-1), 3),\n","            ])\n","            for i, a in enumerate(attendance, start=1):\n","                color = colors.red if a.get('absent') else colors.green\n","                att_style.add('TEXTCOLOR', (1,i), (1,i), color)\n","            att_table.setStyle(att_style)\n","            elements.append(att_table)\n","            elements.append(Spacer(1, 18))\n","\n","        # ====== CLASS EVENTS TABLE (wrapped cells) ======\n","        if timeline_segments:\n","            elements.append(Paragraph(\"Class Events\", styles['Heading3']))\n","            events_data_rows = [[Paragraph('Time', header_style),\n","                                Paragraph('Section', header_style),\n","                                Paragraph('Event', header_style)]]\n","            for seg in timeline_segments:\n","                sec_txt = soft_break_long_token(seg.get('section', '') or '', 14)\n","                evt_txt = soft_break_long_token(seg.get('title', '') or '', 14)\n","                events_data_rows.append([\n","                    _fmt_mmss(seg.get('offset_seconds')),\n","                    Paragraph(sec_txt, contribution_style),\n","                    Paragraph(evt_txt, contribution_style)\n","                ])\n","            # slightly wider Event column to reduce wrapping pressure\n","            events_table = Table(events_data_rows, colWidths=[0.85*inch, 2.10*inch, 4.05*inch], repeatRows=1)\n","            events_table.setStyle(TableStyle([\n","                ('BACKGROUND', (0,0), (-1,0), colors.grey),\n","                ('TEXTCOLOR', (0,0), (-1,0), colors.whitesmoke),\n","                ('ALIGN', (0,0), (-1,0), 'CENTER'),\n","                ('FONTNAME', (0,0), (-1,0), 'Helvetica-Bold'),\n","                ('FONTNAME', (0,1), (-1,-1), 'Helvetica'),\n","                ('FONTSIZE', (0,1), (-1,-1), 10),\n","                ('VALIGN', (0,0), (-1,-1), 'TOP'),\n","                ('GRID', (0,0), (-1,-1), 1, colors.black),\n","                ('LEFTPADDING', (0,0), (-1,-1), 6),\n","                ('RIGHTPADDING', (0,0), (-1,-1), 6),\n","                ('TOPPADDING', (0,0), (-1,-1), 3),\n","                ('BOTTOMPADDING', (0,0), (-1,-1), 3),\n","            ]))\n","            elements.append(events_table)\n","            elements.append(Spacer(1, 18))\n","\n","\n","        # ====== TRANSCRIPT: ONLY BREAK ON NEW CLASS EVENTS ======\n","        elements.append(Paragraph(\"Transcript\", styles['Heading3']))\n","        elements.append(Spacer(1, 6))\n","\n","        # Flatten entries to printable rows\n","        all_items = []\n","        for entry in compiled_entries:\n","            text = ' '.join(entry['text']).strip()\n","            text = normalize_sentence_spacing(text)\n","            if text in ['...', '.', '', 'Mm-hmm.'] or len(text) < 3:\n","                continue\n","            timestamp = _fmt_mmss(entry['start_time'])\n","\n","\n","            max_chars_per_chunk = 500\n","            sentences = text.split('. ')\n","            chunks, curr = [], \"\"\n","            for s in sentences:\n","                candidate = (curr + s + '. ').strip() if curr else (s + '. ')\n","                if len(candidate) <= max_chars_per_chunk:\n","                    curr = candidate\n","                else:\n","                    if curr: chunks.append(curr.strip())\n","                    curr = s + '. '\n","            if curr: chunks.append(curr.strip())\n","\n","            for i, chunk in enumerate(chunks or [text]):\n","                display_ts = \"(cont.)\" if i > 0 else timestamp\n","                all_items.append({\n","                    'start_time': entry['start_time'],\n","                    'end_time': entry['end_time'],\n","                    'timestamp': display_ts,\n","                    'speaker': entry['speaker'],\n","                    'text': chunk\n","                })\n","\n","        all_items.sort(key=lambda x: x['start_time'])\n","\n","        # Build event windows; include preamble\n","        seg_windows = []\n","        if timeline_segments:\n","            first_start = max(0, (timeline_segments[0].get('offset_seconds') or 0))\n","            if first_start > 0:\n","                seg_windows.append({'start': 0, 'end': first_start, 'label': f\"{_fmt_mmss(0)} — Before first event\"})\n","            for idx, seg in enumerate(timeline_segments):\n","                start = max(0, (seg.get('offset_seconds') or 0))\n","                end = (timeline_segments[idx+1].get('offset_seconds') if idx+1 < len(timeline_segments) else float('inf')) or float('inf')\n","                label_bits = []\n","                if seg.get('section'): label_bits.append(seg['section'])\n","                if seg.get('title'): label_bits.append(seg['title'])\n","                label_core = ' · '.join(label_bits) if label_bits else 'Event'\n","                seg_windows.append({'start': start, 'end': end, 'label': f\"{_fmt_mmss(start)} — {label_core}\"})\n","        else:\n","            seg_windows.append({'start': 0, 'end': float('inf'), 'label': \"Transcript\"})\n","\n","        for win in seg_windows:\n","            bucket = [it for it in all_items if win['start'] <= it['start_time'] < win['end']]\n","            if not bucket:\n","                continue\n","\n","            # Event heading inside transcript\n","            elements.append(Paragraph(win['label'], styles['Heading4']))\n","            elements.append(Spacer(1, 4))\n","\n","            # Single table for this event; ReportLab splits across pages automatically.\n","            data = [[Paragraph('Time', header_style),\n","                     Paragraph('Speaker', header_style),\n","                     Paragraph('Contribution', header_style)]]\n","            for item in bucket:\n","                spk_txt = soft_break_long_token(item['speaker'], 14)\n","                data.append([item['timestamp'], Paragraph(spk_txt, speaker_style), Paragraph(normalize_sentence_spacing(item['text']), contribution_style)])\n","\n","            table = Table(data, colWidths=[0.75*inch, 2.2*inch, 4.25*inch], repeatRows=1)\n","            table.setStyle(TableStyle([\n","                ('BACKGROUND', (0, 0), (-1, 0), colors.grey),\n","                ('TEXTCOLOR', (0, 0), (-1, 0), colors.whitesmoke),\n","                ('ALIGN', (0, 0), (-1, 0), 'CENTER'),\n","                ('FONTNAME', (0, 0), (-1, 0), 'Helvetica-Bold'),\n","                ('FONTNAME', (0, 1), (-1, -1), 'Helvetica'),\n","                ('FONTSIZE', (0, 1), (-1, -1), 10),\n","                ('VALIGN', (0, 0), (-1, -1), 'TOP'),\n","                ('GRID', (0, 0), (-1, -1), 1, colors.black),\n","                ('LEFTPADDING', (0, 0), (-1, -1), 6),\n","                ('RIGHTPADDING', (0, 0), (-1, -1), 6),\n","                ('TOPPADDING', (0, 0), (-1, -1), 3),\n","                ('BOTTOMPADDING', (0, 0), (-1, -1), 3),\n","            ]))\n","            elements.append(table)\n","            elements.append(Spacer(1, 16))\n","\n","        doc.build(elements)\n","        print(f\"Created PDF transcript: {output_path}\")\n","        return output_path\n","\n","    except Exception as e:\n","        print(f\"Error processing transcript: {str(e)}\")\n","        raise e\n","\n","def compile_transcript_to_csv(class_id, headers, name_mode: str = \"names\", file_suffix: str = \"\", output_dir: str = \"/content\"):\n","    \"\"\"\n","    CSV:\n","      - Header\n","      - Attendance\n","      - Class Events\n","      - Transcript with event headers\n","    \"\"\"\n","    try:\n","        with open(f\"/content/session_{class_id}_transcript.json\", 'r') as f:\n","            transcript_data = json.load(f)\n","        with open(f\"/content/session_{class_id}_events.json\", 'r') as f:\n","            events_data = json.load(f)\n","\n","        class_meta = events_data.get('class_meta', {})\n","        timeline_segments = events_data.get('timeline_segments', [])\n","        attendance = events_data.get('attendance', [])\n","        student_ids_set = {a.get('id') for a in attendance if a.get('id') is not None}\n","\n","        # Speaker mapping from voice events\n","        speaker_map = {}\n","        for ev in events_data.get('voice_events', []):\n","          speaker_map[(ev['start'], ev['end'])] = ev.get('speaker', {})\n","\n","\n","        def find_speaker_at_time(t):\n","          for (start, end), spk in speaker_map.items():\n","              if start <= t <= end:\n","                  return label_from_actor(spk, name_mode, student_ids_set)\n","          return \"Professor\"\n","\n","        # Combine segments by speaker\n","        compiled_entries = []\n","        current = {'speaker': None, 'start_time': None, 'text': [], 'end_time': None}\n","        for seg in transcript_data['segments']:\n","            st, en, tx = seg['start'], seg['end'], seg['text'].strip()\n","            if not tx:\n","                continue\n","            spk = find_speaker_at_time(st)\n","\n","            start_new = False\n","            if not current['speaker']:\n","                start_new = True\n","            elif current['speaker'] != spk:\n","                start_new = True\n","            elif current['end_time'] is not None and st - current['end_time'] > 2:\n","                start_new = True\n","\n","            if start_new:\n","                if current['speaker']:\n","                    compiled_entries.append(current)\n","                current = {'speaker': spk, 'start_time': st, 'text': [tx], 'end_time': en}\n","            else:\n","                current['text'].append(tx)\n","                current['end_time'] = en\n","        if current['speaker']:\n","            compiled_entries.append(current)\n","\n","        # Flatten → rows\n","        all_items = []\n","        for entry in compiled_entries:\n","            text = normalize_sentence_spacing(' '.join(entry['text']).strip())\n","            if text in ['...', '.', '', 'Mm-hmm.'] or len(text) < 3:\n","                continue\n","            timestamp = _fmt_mmss(entry['start_time'])\n","            all_items.append({\n","                'timestamp': timestamp,\n","                'speaker': entry['speaker'],\n","                'text': text,\n","                'start_time': entry['start_time'],\n","                'end_time': entry['end_time']\n","            })\n","        all_items.sort(key=lambda x: x['start_time'])\n","\n","        # Build event windows (with preamble)\n","        segmented_rows = []\n","        seg_windows = []\n","        if timeline_segments:\n","            first_start = max(0, (timeline_segments[0].get('offset_seconds') or 0))\n","            if first_start > 0:\n","                seg_windows.append({'start': 0, 'end': first_start, 'label': f\"{_fmt_mmss(0)} — Before first event\"})\n","            for idx, seg in enumerate(timeline_segments):\n","                start = max(0, (seg.get('offset_seconds') or 0))\n","                end = (timeline_segments[idx+1].get('offset_seconds') if idx+1 < len(timeline_segments) else float('inf')) or float('inf')\n","                bits = []\n","                if seg.get('section'): bits.append(seg['section'])\n","                if seg.get('title'):   bits.append(seg['title'])\n","                label = f\"{_fmt_mmss(start)} — \" + (' / '.join(bits) if bits else 'Event')\n","                seg_windows.append({'start': start, 'end': end, 'label': label})\n","        else:\n","            seg_windows.append({'start': 0, 'end': float('inf'), 'label': \"Transcript\"})\n","\n","        for win in seg_windows:\n","            bucket = [it for it in all_items if win['start'] <= it['start_time'] < win['end']]\n","            if not bucket:\n","                continue\n","            segmented_rows.append({'timestamp': '', 'speaker': '', 'text': f\"--- {win['label']} ---\"})\n","            segmented_rows.extend(bucket)\n","\n","        all_items = segmented_rows or all_items\n","\n","        # Write CSV\n","        output_path = str(Path(output_dir) / f\"session_{class_id}_transcript{file_suffix}.csv\")\n","        with open(output_path, 'w', newline='', encoding='utf-8') as csvfile:\n","            w = csv.writer(csvfile)\n","\n","            # --- Header block ---\n","            w.writerow([\"Session\", class_meta.get('session_title','')])\n","            sec_sched = class_meta.get('section_title') or class_meta.get('schedule') or ''\n","            if sec_sched:\n","                w.writerow([sec_sched])\n","            w.writerow([])  # blank line (matches PDF spacing)\n","\n","            # class id, class date/time, class link (each on its own labeled line)\n","            class_datetime = _fmt_dt_hm(class_meta.get('recording_start'))\n","            ref = (headers.get('referer') or headers.get('Referer') or '').strip()\n","            m = re.search(r'https://forum\\.minerva\\.edu/app/[^\\s\"\\']+', ref)\n","            class_link = m.group(0) if m else f\"https://forum.minerva.edu/app/classes/{class_id}\"\n","\n","            w.writerow([f\"Class ID: {class_id}\"])\n","            w.writerow([f\"Class Date/Time: {class_datetime}\"])\n","            w.writerow([f\"Class Link: {class_link}\"])\n","            w.writerow([])  # blank line before Attendance / Events\n","\n","            # --- Attendance ---\n","            attendance = events_data.get('attendance', [])\n","            if attendance:\n","                w.writerow([\"Attendance\"])\n","                w.writerow([\"Student\", \"Status\"])\n","                for a in attendance:\n","                    label = (str(a['id']) if (name_mode == \"ids\" and a.get('id')) else a.get('name',''))\n","                    w.writerow([label, \"Absent\" if a.get('absent') else \"Present\"])\n","                w.writerow([])\n","\n","            # --- Class Events (timeline) ---\n","            if timeline_segments:\n","                w.writerow([\"Class Events\"])\n","                w.writerow([\"Time\", \"Section\", \"Event\"])\n","                for seg in timeline_segments:\n","                    w.writerow([\n","                        _fmt_mmss(seg.get('offset_seconds')),\n","                        seg.get('section',''),\n","                        seg.get('title','')\n","                    ])\n","                w.writerow([])\n","\n","            # --- Transcript table ---\n","            w.writerow(['Time', 'Speaker', 'Contribution'])\n","            for item in all_items:\n","                w.writerow([\n","                    item['timestamp'],\n","                    item['speaker'],\n","                    item['text']\n","                ])\n","\n","        print(f\"Created CSV transcript: {output_path}\")\n","        return output_path\n","\n","    except Exception as e:\n","        print(f\"Error creating CSV transcript: {str(e)}\")\n","        return None\n","\n","def create_simplified_csv(class_id, transcript_path, output_dir: str = \"/content\"):\n","    \"\"\"\n","    Fallback CSV: just time + text (no speakers/events).\n","    \"\"\"\n","    try:\n","        with open(f\"/content/session_{class_id}_transcript.json\", 'r') as f:\n","            transcript_data = json.load(f)\n","\n","        output_path = str(Path(output_dir) / f\"session_{class_id}_transcript_simple.csv\")\n","        with open(output_path, 'w', newline='', encoding='utf-8') as csvfile:\n","            writer = csv.writer(csvfile)\n","            writer.writerow(['Time', 'Text'])\n","            for seg in transcript_data['segments']:\n","                minutes = int(seg['start'] // 60)\n","                seconds = int(seg['start'] % 60)\n","                timestamp = f\"{minutes:02d}:{seconds:02d}\"\n","                writer.writerow([timestamp, normalize_sentence_spacing(seg.get('text', ''))])\n","\n","        print(f\"Created simplified CSV transcript: {output_path}\")\n","        return output_path\n","    except Exception as e:\n","        print(f\"Error creating simplified CSV transcript: {str(e)}\")\n","        return None\n","\n","def create_simplified_transcript(class_id, transcript_path, output_dir: str = \"/content\"):\n","    \"\"\"\n","    Fallback PDF: no events, no speakers; includes minimal title.\n","    \"\"\"\n","    try:\n","        from reportlab.lib import colors\n","        from reportlab.lib.pagesizes import letter\n","        from reportlab.platypus import SimpleDocTemplate, Table, TableStyle, Paragraph, Spacer\n","        from reportlab.lib.styles import getSampleStyleSheet, ParagraphStyle\n","        from reportlab.lib.units import inch\n","        import json\n","        from datetime import datetime\n","\n","        with open(f\"/content/session_{class_id}_transcript.json\", 'r') as f:\n","            transcript_data = json.load(f)\n","\n","        styles = getSampleStyleSheet()\n","        text_style = ParagraphStyle('TextStyle', parent=styles['Normal'],\n","                                    fontName='Helvetica', fontSize=10, leading=12, spaceAfter=0, spaceBefore=0,\n","                                    wordWrap='CJK')\n","        header_style = ParagraphStyle('HeaderStyle', parent=styles['Normal'],\n","                                      fontName='Helvetica-Bold', fontSize=12, textColor=colors.whitesmoke,\n","                                      alignment=1)\n","\n","        output_path = str(Path(output_dir) / f\"session_{class_id}_transcript_simple.pdf\")\n","        doc = SimpleDocTemplate(output_path, pagesize=letter,\n","                                rightMargin=72, leftMargin=72, topMargin=72, bottomMargin=72)\n","\n","        elements = []\n","        title = Paragraph(f\"Session {class_id}\", styles['Title'])\n","        date_str = datetime.now().strftime(\"%Y-%m-%d\")\n","        subtitle = Paragraph(f\"Generated on {date_str}\", styles['Heading2'])\n","        elements.append(title)\n","        elements.append(subtitle)\n","        elements.append(Spacer(1, 12))\n","\n","        data = [[Paragraph('Time', header_style), Paragraph('Text', header_style)]]\n","        for seg in transcript_data['segments']:\n","            minutes = int(seg['start'] // 60)\n","            seconds = int(seg['start'] % 60)\n","            timestamp = f\"{minutes:02d}:{seconds:02d}\"\n","            data.append([\n","                timestamp,\n","                Paragraph(normalize_sentence_spacing(seg['text']), text_style)\n","            ])\n","\n","        table = Table(data, colWidths=[0.75*inch, 6.25*inch], repeatRows=1)\n","        table.setStyle(TableStyle([\n","            ('BACKGROUND', (0, 0), (-1, 0), colors.grey),\n","            ('TEXTCOLOR', (0, 0), (-1, 0), colors.whitesmoke),\n","            ('ALIGN', (0, 0), (-1, 0), 'CENTER'),\n","            ('FONTNAME', (0, 0), (-1, 0), 'Helvetica-Bold'),\n","            ('FONTNAME', (0, 1), (-1, -1), 'Helvetica'),\n","            ('FONTSIZE', (0, 1), (-1, -1), 10),\n","            ('VALIGN', (0, 0), (-1, -1), 'TOP'),\n","            ('GRID', (0, 0), (-1, -1), 1, colors.black),\n","            ('LEFTPADDING', (0, 0), (-1, -1), 6),\n","            ('RIGHTPADDING', (0, 0), (-1, -1), 6),\n","            ('TOPPADDING', (0, 0), (-1, -1), 3),\n","            ('BOTTOMPADDING', (0, 0), (-1, -1), 3),\n","        ]))\n","        elements.append(table)\n","        doc.build(elements)\n","        print(f\"Created simplified PDF transcript: {output_path}\")\n","        return output_path\n","    except Exception as e:\n","        print(f\"Error creating simplified transcript: {str(e)}\")\n","        return None"]},{"cell_type":"code","source":["def _emit_outputs(class_id, headers, privacy_mode: str, output_dir: str):\n","    \"\"\"\n","    Builds PDF/CSV for names, ids, or both, into the given output_dir.\n","    Returns a dict of produced file paths.\n","    \"\"\"\n","    outputs = {}\n","    if privacy_mode == \"both\":\n","        # Names versions\n","        outputs['pdf_names'] = compile_transcript_to_pdf(class_id, headers, name_mode=\"names\", file_suffix=\"_names\", output_dir=output_dir)\n","        outputs['csv_names'] = compile_transcript_to_csv(class_id, headers, name_mode=\"names\", file_suffix=\"_names\", output_dir=output_dir)\n","        # IDs versions\n","        outputs['pdf_ids'] = compile_transcript_to_pdf(class_id, headers, name_mode=\"ids\", file_suffix=\"_ids\", output_dir=output_dir)\n","        outputs['csv_ids'] = compile_transcript_to_csv(class_id, headers, name_mode=\"ids\", file_suffix=\"_ids\", output_dir=output_dir)\n","    else:\n","        suffix = \"\" if privacy_mode == \"names\" else \"_ids\"\n","        outputs['pdf'] = compile_transcript_to_pdf(class_id, headers, name_mode=privacy_mode, file_suffix=suffix, output_dir=output_dir)\n","        outputs['csv'] = compile_transcript_to_csv(class_id, headers, name_mode=privacy_mode, file_suffix=suffix, output_dir=output_dir)\n","    return outputs\n","\n","def _print_outputs(outputs: dict, privacy_mode: str):\n","    \"\"\"Console-print produced file paths; supports single or 'both' privacy modes.\"\"\"\n","    if privacy_mode == \"both\":\n","        print(\"\\nSuccess! Your transcripts are ready (both versions):\")\n","        print(f\"PDF (names): {outputs.get('pdf_names')}\")\n","        print(f\"CSV (names): {outputs.get('csv_names')}\")\n","        print(f\"PDF (ids):   {outputs.get('pdf_ids')}\")\n","        print(f\"CSV (ids):   {outputs.get('csv_ids')}\")\n","    else:\n","        print(\"\\nSuccess! Your transcripts are ready:\")\n","        print(f\"PDF: {outputs.get('pdf')}\")\n","        print(f\"CSV: {outputs.get('csv')}\")"],"metadata":{"id":"JPIaN7HMyJGJ"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["<div style=\"border:2px solid #D35400;padding:12px;border-radius:8px;background:#FFF7F0\">\n","<h3>Pipeline Entrypoint</h3>\n","<p>\n","Coordinates the full run: fetch Forum data → preprocess audio → transcribe → compile outputs. Prints final paths and a caution to double-check accuracy.\n","</p>\n","<ul>\n","  <li>Handles local / Drive / URL inputs and local / Drive outputs.</li>\n","  <li>If “both” privacy is selected, produces both <code>_names</code> and <code>_ids</code> versions.</li>\n","</ul>\n","</div>"],"metadata":{"id":"E-rpmXvYBoHh"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"Dbtt5NLEYHvL"},"outputs":[],"source":["# @title\n","def process_lecture(audio_path, class_id, curl_string):\n","    \"\"\"\n","    End-to-end pipeline with graceful fallbacks.\n","    \"\"\"\n","    output_pdf, output_csv = None, None  # NEW: ensure defined for the return\n","    try:\n","        # 1) Forum events\n","        print(\"Step 1/4: Processing Forum class events...\")\n","        headers = clean_curl(curl_string)\n","        events_data = get_forum_events(class_id, headers)\n","\n","        # 2) Audio preprocess\n","        print(\"\\nStep 2/4: Preprocessing audio file...\")\n","        preprocessor = AudioPreprocessor()\n","        fixed_path = preprocessor.validate_and_fix_file(audio_path)\n","\n","        # 3) Transcribe\n","        print(\"\\nStep 3/4: Generating transcript...\")\n","        tp = TranscriptionProcessor()\n","        transcript_path = tp.transcribe(fixed_path, class_id)\n","\n","        # 4) Compile outputs\n","        print(\"\\nStep 4/4: Compiling final PDF and CSV transcripts...\")\n","        try:\n","            outputs = _emit_outputs(class_id, headers, PRIVACY_MODE, OUTPUT_DIR)\n","            _print_outputs(outputs, PRIVACY_MODE)\n","\n","            # NEW: choose representative return values\n","            if PRIVACY_MODE == \"both\":\n","                output_pdf = outputs.get('pdf_names') or outputs.get('pdf_ids')\n","                output_csv = outputs.get('csv_names') or outputs.get('csv_ids')\n","            else:\n","                output_pdf = outputs.get('pdf')\n","                output_csv = outputs.get('csv')\n","\n","            # NEW: accuracy caution banner\n","            print(\"\\n⚠️  Accuracy caution: Do not rely solely on this transcript. Manually verify key information.\")\n","\n","        except Exception as e:\n","            print(f\"Error compiling transcripts: {str(e)}\")\n","            print(\"Attempting to create simplified transcripts without speaker information...\")\n","            # Simplified outputs don't include names/attendance, so privacy mode is irrelevant here\n","            output_pdf = create_simplified_transcript(class_id, transcript_path, OUTPUT_DIR)\n","            output_csv = create_simplified_csv(class_id, transcript_path, OUTPUT_DIR)\n","            if output_pdf and output_csv:\n","              print(f\"\\nCreated simplified transcripts:\")\n","              print(f\"PDF: {output_pdf}\")\n","              print(f\"CSV: {output_csv}\")\n","              print(\"\\n⚠️  Accuracy caution: Do not rely solely on this transcript. Manually verify key information.\")\n","\n","            else:\n","                print(\"Failed to create simplified transcripts.\")\n","                return None, None\n","\n","        # Cleanup converted audio\n","        try:\n","            temp_files = [fixed_path]\n","            for temp_file in temp_files:\n","                if temp_file != audio_path and Path(temp_file).exists():\n","                    Path(temp_file).unlink()\n","                    print(f\"Cleaned up temporary file: {temp_file}\")\n","        except Exception as cleanup_error:\n","            print(f\"Note: Could not clean up temporary files: {str(cleanup_error)}\")\n","\n","        return output_pdf, output_csv\n","\n","    except Exception as e:\n","        print(f\"\\nERROR: {str(e)}\")\n","        if \"MP4\" in str(e) and audio_path.lower().endswith('.mp4'):\n","            print(\"\\nThere was a problem with your MP4 file. Suggestions:\")\n","            print(\"1. Convert it to MP3 on your computer before uploading\")\n","            print(\"2. Use a screen recorder to record Forum while playing back the class\")\n","            print(\"3. Contact Forum support about MP4 download issues\")\n","        else:\n","            print(\"\\nTranscription failed. Please try again with a different file.\")\n","        return None, None"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"hTsg4BCLYKRM","colab":{"base_uri":"https://localhost:8080/","height":639,"referenced_widgets":["51cbc42b883c44e59a47ae3585722cb9","e558d4047b9341898dec75d39d6e6c5f","17aef962596146f6842f786916ce8a3d","ab13fb2d736b418e92a0f4d5aef33c55","d4164aae17474e06b5fb3ded12acb5a5","022423d9bdbc4c1cb4c030b6d4c6f784","97f6b6e975014202845b02c863b88c93","728c45925b84404aa6657f615ddd956a","7bcd0bef8c2c4b7795d82aa2eb9000db","e7616fdf0368456ba6b7285146c57693","70848e006095459982b060af45a55449"]},"executionInfo":{"status":"ok","timestamp":1755477236459,"user_tz":180,"elapsed":610514,"user":{"displayName":"Aleksei Korablev","userId":"02837062072924939756"}},"outputId":"76b1aa8a-27fc-49d1-cf0e-efd700578889"},"outputs":[{"output_type":"stream","name":"stdout","text":["Step 1/4: Processing Forum class events...\n","Fetching class and event data from Forum...\n","Requesting class data from: https://forum.minerva.edu/api/v1/class_grader/classes/81239\n","Requesting events from: https://forum.minerva.edu/api/v1/class_grader/classes/81239/class-events\n","Processed voice events: 126; timeline segments: 15\n","\n","Step 2/4: Preprocessing audio file...\n","Validating file: /content/input_from_url.mp4\n","Converting MP4 to MP3 (intermediate step)...\n","Successfully converted to MP3: /content/input_from_url.mp3\n","Successfully converted to Whisper-optimized WAV: /content/input_from_url.wav\n","\n","Step 3/4: Generating transcript...\n","Using device: cuda\n","Loading Whisper model...\n","Processing audio to generate transcript JSON...\n","Total duration: 1:41:00\n"]},{"output_type":"display_data","data":{"text/plain":["Processing segments:   0%|          | 0/1 [00:00<?, ?segment/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"51cbc42b883c44e59a47ae3585722cb9"}},"metadata":{}},{"output_type":"stream","name":"stdout","text":["\n","Transcript JSON saved to: /content/session_81239_transcript.json\n","\n","Step 4/4: Compiling final PDF and CSV transcripts...\n","Created PDF transcript: /content/drive/MyDrive/AI Curriculum Projects Summer 2025/Transcriptions/Alex/v2.3/session_81239_transcript_names.pdf\n","Created CSV transcript: /content/drive/MyDrive/AI Curriculum Projects Summer 2025/Transcriptions/Alex/v2.3/session_81239_transcript_names.csv\n","Created PDF transcript: /content/drive/MyDrive/AI Curriculum Projects Summer 2025/Transcriptions/Alex/v2.3/session_81239_transcript_ids.pdf\n","Created CSV transcript: /content/drive/MyDrive/AI Curriculum Projects Summer 2025/Transcriptions/Alex/v2.3/session_81239_transcript_ids.csv\n","\n","Success! Your transcripts are ready (both versions):\n","PDF (names): /content/drive/MyDrive/AI Curriculum Projects Summer 2025/Transcriptions/Alex/v2.3/session_81239_transcript_names.pdf\n","CSV (names): /content/drive/MyDrive/AI Curriculum Projects Summer 2025/Transcriptions/Alex/v2.3/session_81239_transcript_names.csv\n","PDF (ids):   /content/drive/MyDrive/AI Curriculum Projects Summer 2025/Transcriptions/Alex/v2.3/session_81239_transcript_ids.pdf\n","CSV (ids):   /content/drive/MyDrive/AI Curriculum Projects Summer 2025/Transcriptions/Alex/v2.3/session_81239_transcript_ids.csv\n","\n","⚠️  Accuracy caution: Do not rely solely on this transcript. Manually verify key information.\n","Cleaned up temporary file: /content/input_from_url.wav\n"]}],"source":["# Run the process\n","pdf_output, csv_output = process_lecture(AUDIO_PATH, CLASS_ID, raw_curl)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"p3BH0hikrkBQ","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1755477236934,"user_tz":180,"elapsed":470,"user":{"displayName":"Aleksei Korablev","userId":"02837062072924939756"}},"outputId":"025b6496-bb3a-497c-bef3-6d2812e6e68f"},"outputs":[{"output_type":"stream","name":"stdout","text":["CUDA cache cleared\n"]}],"source":["free_cuda_mem()\n","\n","print(\"CUDA cache cleared\")"]}],"metadata":{"accelerator":"GPU","colab":{"gpuType":"T4","provenance":[{"file_id":"1LcaKeGSpPnqXNMHZBX7FPzIo4bAmRuNP","timestamp":1754999391243},{"file_id":"1T3Jbu9KmoglVhKMorGYH1eikq1psXW0x","timestamp":1753457099254}]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"},"widgets":{"application/vnd.jupyter.widget-state+json":{"51cbc42b883c44e59a47ae3585722cb9":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_e558d4047b9341898dec75d39d6e6c5f","IPY_MODEL_17aef962596146f6842f786916ce8a3d","IPY_MODEL_ab13fb2d736b418e92a0f4d5aef33c55"],"layout":"IPY_MODEL_d4164aae17474e06b5fb3ded12acb5a5"}},"e558d4047b9341898dec75d39d6e6c5f":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_022423d9bdbc4c1cb4c030b6d4c6f784","placeholder":"​","style":"IPY_MODEL_97f6b6e975014202845b02c863b88c93","value":"Processing segments: 100%"}},"17aef962596146f6842f786916ce8a3d":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_728c45925b84404aa6657f615ddd956a","max":1,"min":0,"orientation":"horizontal","style":"IPY_MODEL_7bcd0bef8c2c4b7795d82aa2eb9000db","value":1}},"ab13fb2d736b418e92a0f4d5aef33c55":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_e7616fdf0368456ba6b7285146c57693","placeholder":"​","style":"IPY_MODEL_70848e006095459982b060af45a55449","value":" 1/1 [07:56&lt;00:00, 476.86s/segment]"}},"d4164aae17474e06b5fb3ded12acb5a5":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"022423d9bdbc4c1cb4c030b6d4c6f784":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"97f6b6e975014202845b02c863b88c93":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"728c45925b84404aa6657f615ddd956a":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"7bcd0bef8c2c4b7795d82aa2eb9000db":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"e7616fdf0368456ba6b7285146c57693":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"70848e006095459982b060af45a55449":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}}}}},"nbformat":4,"nbformat_minor":0}